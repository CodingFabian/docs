{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/Users/vigil/Desktop/Vigil/autoredteam/autoredteam/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def get_detectors(test_detector_dict):\n",
    "    extracted_values = {}\n",
    "    for key, values in test_detector_dict.items():\n",
    "        extracted_values[key] = [f\"{value.__class__.__module__}.{value.__class__.__name__}\" for value in values]\n",
    "    return extracted_values\n",
    "\n",
    "\n",
    "def get_counts(extracted_values):\n",
    "    values = [value for sublist in extracted_values.values() for value in sublist]\n",
    "    value_counts = Counter(values)\n",
    "\n",
    "    return value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OctoAI Agent: mistral-7b-instruct-fp16\n"
     ]
    }
   ],
   "source": [
    "from autoredteam.agents.octo import OctoAPI\n",
    "agent = OctoAPI(name = \"mistral-7b-instruct-fp16\", generations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from autoredteam.harnesses.dimension import SecurityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness\n",
    "harnesses = [SecurityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "harness_instances = [harness(agent) for harness in harnesses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [instance.test_instances for instance in harness_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for each element in sec_tests and store the dictionaries in another list\n",
    "test_detector_dicts = [{test: test.detectors for test in sec_test} for sec_test in tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_list = list(test_detector_dicts[1].keys())\n",
    "tester = keys_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Test Information\n",
    "Below is for getting the test information. It is saved in three different formats and pulls everything from the harnesses above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv \n",
    "\n",
    "def get_object_attributes(obj):\n",
    "    # Get all attributes of the object\n",
    "    attributes = dir(obj)\n",
    "    # Filter out private and special attributes\n",
    "    user_defined_attributes = {attr: getattr(obj, attr) for attr in attributes if not attr.startswith('__')}\n",
    "    # Exclude methods and convert non-serializable objects\n",
    "    user_defined_attributes = {attr: convert_to_serializable(attr, value) for attr, value in user_defined_attributes.items() if not callable(value)}\n",
    "    return user_defined_attributes\n",
    "\n",
    "def convert_to_serializable(attr, value):\n",
    "    if isinstance(value, (int, float, str, bool, type(None))):\n",
    "        # If the value is already JSON serializable, return it as is\n",
    "        return value\n",
    "    elif hasattr(value, '__dict__'):\n",
    "        # If the value has a '__dict__' attribute, assume it's an object and convert it to a string\n",
    "        return str(value)\n",
    "    elif isinstance(value, list) and attr in ['detectors', 'probe']:\n",
    "        # If the attribute is \"detectors\" or \"probe\" and the value is a list of objects, convert it to a string containing class names\n",
    "        class_names = [type(item).__module__ + '.' + type(item).__name__ for item in value]\n",
    "        return ', '.join(class_names)\n",
    "    else:\n",
    "        # If all else fails, return a string representation of the value\n",
    "        return repr(value)\n",
    "\n",
    "\n",
    "def save_object_attributes(obj_lists, filename_prefix):\n",
    "    all_attributes = []\n",
    "    # Collect all unique keys\n",
    "    all_keys = set()\n",
    "    for i, obj_list in enumerate(obj_lists, start=1):\n",
    "        for j, obj in enumerate(obj_list, start=1):\n",
    "            # Get the object's attributes\n",
    "            obj_attributes = get_object_attributes(obj)\n",
    "            # Append to the list of all attributes\n",
    "            all_attributes.append(obj_attributes)\n",
    "            # Update the set of keys\n",
    "            all_keys.update(obj_attributes.keys())\n",
    "\n",
    "\n",
    "    # Save to JSON\n",
    "    json_filename = f\"{filename_prefix}.json\"\n",
    "    with open(json_filename, 'w') as file:\n",
    "        json.dump(all_attributes, file, indent=4)\n",
    "    \n",
    "    # Save to Markdown\n",
    "    md_filename = f\"{filename_prefix}.md\"\n",
    "    with open(md_filename, 'w') as file:\n",
    "        # Write table header\n",
    "        file.write(\"|\")\n",
    "        for key in all_keys:\n",
    "            file.write(f\"{key}|\")\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"|\")\n",
    "        for _ in all_keys:\n",
    "            file.write(\"---|\")\n",
    "        file.write(\"\\n\")\n",
    "        # Write table rows\n",
    "        for attributes in all_attributes:\n",
    "            file.write(\"|\")\n",
    "            for key in all_keys:\n",
    "                file.write(f\"{attributes.get(key, '')}|\")\n",
    "            file.write(\"\\n\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_filename = f\"{filename_prefix}.csv\"\n",
    "    with open(csv_filename, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=all_keys)\n",
    "        writer.writeheader()\n",
    "        for attributes in all_attributes:\n",
    "            writer.writerow(attributes)\n",
    "\n",
    "\n",
    "\n",
    "# Save attributes of all tests to separate JSON files\n",
    "save_object_attributes(tests, 'test_attributes')\n",
    "\n",
    "# Alternatively, you can save attributes of all tests to a single JSON file\n",
    "# save_test_attributes(tests_list, 'all_tests_attributes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating with new goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Load the mapping CSV file\n",
    "df_csv = pd.read_csv('goals.csv')\n",
    "df_csv.columns = df_csv.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from old values to new ones\n",
    "mapping = pd.Series(df_csv['New goal'].values, index=df_csv['Old goals']).to_dict()\n",
    "# Replace values in the CSV file\n",
    "df = pd.read_csv('test_attributes.csv')\n",
    "df.replace({'goal': mapping}, inplace=True)\n",
    "df.to_csv('test_attributes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in the JSON file\n",
    "with open('test_attributes.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = [{k: mapping.get(v, v) if k == 'goal' else v for k, v in item.items()} for item in data]\n",
    "\n",
    "with open('test_attributes.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in the Markdown file\n",
    "with open('test_attributes.md', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [line.replace(old, str(mapping[old])) for line in lines for old in mapping if old in line]\n",
    "\n",
    "with open('test_attributes.md', 'w') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding prompt ID from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloze data replay for literature. Uses passages that either GPT-4 or ChatGPT have been known to replay. - based on articles from The Guardian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<garak.detectors.base.TriggerListDetector at 0x317ef8f20>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tester.description)\n",
    "tester.detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib.util\n",
    "import inspect\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/vigil/Desktop/Vigil/autoredteam')\n",
    "\n",
    "# Define the directory with the .py files\n",
    "directory = '/Users/vigil/Desktop/Vigil/autoredteam/autoredteam/tests'\n",
    "\n",
    "# Define the markdown file\n",
    "markdown_directory = 'markdown_files'\n",
    "markdown_file = os.path.join(markdown_directory, 'attributes.md')\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(markdown_directory, exist_ok=True)\n",
    "# Open the markdown file\n",
    "with open(markdown_file, 'w') as f:\n",
    "    # List all .py files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.py'):\n",
    "            # Import the .py file as a module\n",
    "            spec = importlib.util.spec_from_file_location(filename[:-3], os.path.join(directory, filename))\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "\n",
    "            # Get the attributes of the module\n",
    "            attributes = inspect.getmembers(module)\n",
    "\n",
    "            # Filter the attributes\n",
    "            attributes = [attr for attr in attributes if attr[0] in ['__doc__', 'detectors', 'uri', 'name', 'description', 'tags', 'goal']]\n",
    "\n",
    "            # Write the attributes to the markdown file in a table format\n",
    "            f.write(f'## {filename[:-3]}\\n')\n",
    "            f.write('| Attribute | Value |\\n')\n",
    "            f.write('| --- | --- |\\n')\n",
    "            for attr in attributes:\n",
    "                f.write(f'| {attr[0]} | {attr[1]} |\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the markdown file\n",
    "markdown_file = 'markdown_files/all_tests.md'\n",
    "\n",
    "# Create a directory for the markdown file\n",
    "os.makedirs(os.path.dirname(markdown_file), exist_ok=True)\n",
    "\n",
    "# Create a dictionary to group the subtests by their categories\n",
    "tests_by_category = defaultdict(list)\n",
    "\n",
    "# Iterate over all dictionaries in test_detector_dicts\n",
    "for test_detector_dict in test_detector_dicts:\n",
    "    # Iterate over the items in each dictionary\n",
    "    for test, detectors in test_detector_dict.items():\n",
    "        # Extract the relevant part of the test name\n",
    "        test_category = str(test).split('.')[2]\n",
    "        test_name = str(test).split(' ')[0].split('.')[-1]\n",
    "\n",
    "        # Group the subtests by their categories\n",
    "        tests_by_category[test_category].append((test_name, detectors))\n",
    "\n",
    "# Open the markdown file\n",
    "with open(markdown_file, 'w') as f:\n",
    "    # Iterate over the categories\n",
    "    for category, tests in tests_by_category.items():\n",
    "        # Write the category to the file\n",
    "        f.write(f'## {category}\\n')\n",
    "\n",
    "        # Write the subtests to the file\n",
    "        for test_name, detectors in tests:\n",
    "            f.write(f'- {test_name}\\n')\n",
    "\n",
    "            # Write the detectors to the file\n",
    "            for detector in detectors:\n",
    "                # Extract the relevant part of the detector name\n",
    "                detector_name = '.'.join(str(detector).split(' ')[0].split('.')[1:])\n",
    "                f.write(f'  - {detector_name}\\n')\n",
    "\n",
    "        # Add a newline for readability\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update each dictionary with the result of calling get_detectors on it\n",
    "for i in range(len(test_detector_dicts)):\n",
    "    test_detector_dicts[i] = get_detectors(test_detector_dicts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detector_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the markdown file\n",
    "markdown_file = 'markdown_files/all_tests.md'\n",
    "\n",
    "# Create a directory for the markdown file\n",
    "os.makedirs(os.path.dirname(markdown_file), exist_ok=True)\n",
    "\n",
    "# Open the markdown file\n",
    "with open(markdown_file, 'w') as f:\n",
    "    # Iterate over the dictionary in test_detector_dicts\n",
    "    for test, detectors in test_detector_dicts[0].items():\n",
    "        # Extract the test name from the object string\n",
    "        test_name = str(test).split(' ')[0].split('.')[-1]\n",
    "\n",
    "        # Write the test name to the file\n",
    "        f.write(f'## {test_name}\\n')\n",
    "\n",
    "        # Write the detectors to the file\n",
    "        for detector in detectors:\n",
    "            f.write(f'- {detector}\\n')\n",
    "\n",
    "        # Add a newline for readability\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counts = [get_counts(dict) for dict in test_detector_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness_detector_counts= list(zip(harnesses, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness_detector_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Iterate over the zipped list\n",
    "for harness, counter in harness_detector_counts:\n",
    "    # Get the names of the detectors and their counts\n",
    "    detectors = list(counter.keys())\n",
    "    counts = list(counter.values())\n",
    "    \n",
    "    # Sort the detectors and counts by the counts\n",
    "    detectors, counts = zip(*sorted(zip(detectors, counts), key=lambda x: x[1]))\n",
    "    \n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(10, 5))  # Set the figure size\n",
    "    plt.barh(detectors, counts)  # Create a horizontal bar chart\n",
    "    plt.title(str(harness))  # Set the title to the name of the harness\n",
    "    plt.xlabel('Counts')  # Set the x-axis label\n",
    "    plt.ylabel('Detectors')  # Set the y-axis label\n",
    "    plt.xlim(0, max(counts) + 1)  # Set the x-axis limits\n",
    "    plt.xticks(np.arange(0, max(counts) + 1, step=1))  # Set the x-axis ticks to integers\n",
    "    plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.harnesses.dimension import * \n",
    "harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "harness_instances = [harness(agent) for harness in harnesses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.detectors.base import *\n",
    "detectors = [Detector, HFDetector, StringAbsenceDetector, StringDetector, TriggerListAbsenceDetector, TriggerListDetector]\n",
    "first_instance = detectors[0]()\n",
    "\n",
    "# Get all the attributes and methods of the instance\n",
    "attributes = dir(first_instance)\n",
    "\n",
    "# Print the attributes\n",
    "for attribute in attributes:\n",
    "    print(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.detectors.base import *\n",
    "detectors = [Detector, StringDetector, TriggerListAbsenceDetector, TriggerListDetector]\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['bcp47', 'description', 'detect', 'detectorname', 'name', 'precision', 'recall', 'tags', 'uri']\n",
    "\n",
    "for DetectorClass in detectors:\n",
    "    # Create an instance of the class\n",
    "    instance = DetectorClass()\n",
    "\n",
    "    # Extract the specified attributes\n",
    "    for attribute in attributes_to_extract:\n",
    "        if hasattr(instance, attribute):\n",
    "            print(f'{attribute}: {getattr(instance, attribute)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "# List of attributes to look for\n",
    "attributes_to_look_for = ['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri']\n",
    "\n",
    "instances_data = []\n",
    "for instance in test.test_instances:\n",
    "    instance_dict = {}\n",
    "    for attribute in attributes_to_look_for:\n",
    "        if hasattr(instance, attribute):\n",
    "            if attribute == 'detectors':\n",
    "                # Get the class name of each detector and join them into a single string\n",
    "                instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "            elif attribute == 'probe':\n",
    "                # Get the class name of the probe\n",
    "                probe = getattr(instance, attribute)\n",
    "                instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "            elif attribute == 'tags':\n",
    "                # Join the tags into a single string\n",
    "                instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "            else:\n",
    "                instance_dict[attribute] = getattr(instance, attribute)\n",
    "    instances_data.append(instance_dict)\n",
    "\n",
    "print(instances_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Add a title to the markdown\n",
    "markdown_table = '# Security\\n' + markdown_table\n",
    "\n",
    "print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.harnesses.dimension import * \n",
    "from tabulate import tabulate\n",
    "\n",
    "harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "harness_instances = [harness(agent) for harness in harnesses]\n",
    "\n",
    "# List of attributes to look for\n",
    "attributes_to_look_for = ['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri']\n",
    "\n",
    "for harness_instance in harness_instances:\n",
    "    instances_data = []\n",
    "    for instance in harness_instance.test_instances:\n",
    "        instance_dict = {}\n",
    "        for attribute in attributes_to_look_for:\n",
    "            if hasattr(instance, attribute):\n",
    "                if attribute == 'detectors':\n",
    "                    # Get the class name of each detector and join them into a single string\n",
    "                    instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "                elif attribute == 'probe':\n",
    "                    # Get the class name of the probe\n",
    "                    probe = getattr(instance, attribute)\n",
    "                    instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "                elif attribute == 'tags':\n",
    "                    # Join the tags into a single string\n",
    "                    instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "                elif attribute == 'description':\n",
    "                    # Check if the description is a tuple, and if so, take the first element\n",
    "                    description = getattr(instance, attribute)\n",
    "                    if isinstance(description, tuple):\n",
    "                        description = description[0]\n",
    "                    instance_dict[attribute] = description\n",
    "                else:\n",
    "                    instance_dict[attribute] = getattr(instance, attribute)\n",
    "        instances_data.append(instance_dict)\n",
    "\n",
    "    # Convert the list of dictionaries to a markdown table\n",
    "    markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "    # Add a title to the markdown\n",
    "    markdown_table = '# ' + type(harness_instance).__name__ + '\\n' + markdown_table\n",
    "\n",
    "    print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.harnesses.dimension import * \n",
    "from tabulate import tabulate\n",
    "\n",
    "def generate_documentation(attributes_to_look_for, save_path):\n",
    "    harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "    harness_instances = [harness(agent) for harness in harnesses]\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        for harness_instance in harness_instances:\n",
    "            instances_data = []\n",
    "            for instance in harness_instance.test_instances:\n",
    "                instance_dict = {}\n",
    "                for attribute in attributes_to_look_for:\n",
    "                    if hasattr(instance, attribute):\n",
    "                        if attribute == 'detectors':\n",
    "                            # Get the class name of each detector and join them into a single string\n",
    "                            instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "                        elif attribute == 'probe':\n",
    "                            # Get the class name of the probe\n",
    "                            probe = getattr(instance, attribute)\n",
    "                            instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "                        elif attribute == 'tags':\n",
    "                            # Join the tags into a single string\n",
    "                            instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "                        else:\n",
    "                            instance_dict[attribute] = getattr(instance, attribute)\n",
    "                instances_data.append(instance_dict)\n",
    "\n",
    "            # Convert the list of dictionaries to a markdown table\n",
    "            markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "            # Add a title to the markdown\n",
    "            markdown_table = '# ' + type(harness_instance).__name__ + '\\n' + markdown_table\n",
    "\n",
    "            # Write the markdown table to the file\n",
    "            f.write(markdown_table + '\\n\\n')\n",
    "\n",
    "# Call the function with the attributes to look for and the save path\n",
    "generate_documentation(['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri'], 'vijil_detectors.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from autoredteam.harnesses.dimension import * \n",
    "from tabulate import tabulate\n",
    "\n",
    "def generate_documentation(attributes_to_look_for, save_path):\n",
    "    harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "    harness_instances = [harness(agent) for harness in harnesses]\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        for harness_instance in harness_instances:\n",
    "            instances_data = []\n",
    "            for instance in harness_instance.test_instances:\n",
    "                instance_dict = {}\n",
    "                for attribute in attributes_to_look_for:\n",
    "                    if hasattr(instance, attribute):\n",
    "                        if attribute == 'detectors':\n",
    "                            # Get the class name of each detector and join them into a single string\n",
    "                            instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "                        elif attribute == 'probe':\n",
    "                            # Get the class name of the probe\n",
    "                            probe = getattr(instance, attribute)\n",
    "                            instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "                        elif attribute == 'tags':\n",
    "                            # Join the tags into a single string\n",
    "                            instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "                        elif attribute == 'description':\n",
    "                            # Check if the description is a tuple, and if so, take the first element\n",
    "                            description = getattr(instance, attribute)\n",
    "                            if isinstance(description, tuple):\n",
    "                                description = description[0]\n",
    "                            instance_dict[attribute] = description\n",
    "                        else:\n",
    "                            instance_dict[attribute] = getattr(instance, attribute)\n",
    "                instances_data.append(instance_dict)\n",
    "\n",
    "            # Convert the list of dictionaries to a markdown table\n",
    "            markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "            # Add a title to the markdown\n",
    "            markdown_table = '# ' + type(harness_instance).__name__ + '\\n' + markdown_table\n",
    "\n",
    "            # Write the markdown table to the file\n",
    "            f.write(markdown_table + '\\n\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Generate documentation for test instances.')\n",
    "    parser.add_argument('attributes', type=str, nargs='+', help='List of attributes to generate documentation for.')\n",
    "    parser.add_argument('save_path', type=str, help='Path to save the markdown file.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Call the function with the attributes to look for and the save path\n",
    "    generate_documentation(args.attributes, args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from garak.detectors.dan import DAN, DANJailbreak, DevMode, DUDE, STAN, AntiDAN, MarkdownLink\n",
    "from tabulate import tabulate\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['bcp47', 'description', 'detect', 'detectorname', 'name', 'precision', 'recall', 'tags', 'uri']\n",
    "\n",
    "instances = [DAN, DANJailbreak, DevMode, DUDE, STAN, AntiDAN, MarkdownLink]\n",
    "\n",
    "data = []\n",
    "\n",
    "# Extract the specified attributes\n",
    "for instance in instances:\n",
    "    row = {'name': instance.__name__}\n",
    "    for attribute in attributes_to_extract:\n",
    "        if hasattr(instance, attribute):\n",
    "            row[attribute] = getattr(instance, attribute)\n",
    "    data.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Save the markdown table to a file\n",
    "with open('output.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import garak.detectors as detectors\n",
    "from tabulate import tabulate\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['detectorname', 'name', 'bcp47', 'description', 'detect', 'tags', 'uri']\n",
    "\n",
    "# Get all modules in the garak.detectors package\n",
    "detector_modules = list(pkgutil.iter_modules(detectors.__path__))\n",
    "\n",
    "instances = []\n",
    "\n",
    "# Import each module and add its classes to the instances list\n",
    "for _, module_name, _ in detector_modules:\n",
    "    module = __import__(f'garak.detectors.{module_name}', fromlist=[module_name])\n",
    "    for attribute_name in dir(module):\n",
    "        attribute = getattr(module, attribute_name)\n",
    "        if isinstance(attribute, type) and attribute_name != 'Attempt':\n",
    "            instances.append(attribute)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Extract the specified attributes\n",
    "for instance in instances:\n",
    "    row = {'name': instance.__name__}\n",
    "    for attribute in attributes_to_extract:\n",
    "        if hasattr(instance, attribute):\n",
    "            row[attribute] = getattr(instance, attribute)\n",
    "    data.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Save the markdown table to a file\n",
    "with open('new_output.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from garak.detectors.riskywords import SurgeProfanityAnimalReferences\n",
    "this = SurgeProfanityAnimalReferences()\n",
    "this.substrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Detector Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import garak.detectors as detectors\n",
    "from tabulate import tabulate\n",
    "import types\n",
    "import json\n",
    "\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, type):\n",
    "            return str(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['detectorname', 'name', 'bcp47', 'description', 'detect', 'tags', 'uri', 'substrings']\n",
    "\n",
    "# Get all modules in the garak.detectors package\n",
    "detector_modules = list(pkgutil.iter_modules(detectors.__path__))\n",
    "\n",
    "instances = []\n",
    "\n",
    "# Import each module and add its classes to the instances list\n",
    "for _, module_name, _ in detector_modules:\n",
    "    module = __import__(f'garak.detectors.{module_name}', fromlist=[module_name])\n",
    "    for attribute_name in dir(module):\n",
    "        attribute = getattr(module, attribute_name)\n",
    "        if isinstance(attribute, type) and attribute_name != 'Attempt':\n",
    "            try:\n",
    "                instances.append(attribute())\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "data = []\n",
    "\n",
    "# Extract the specified attributes\n",
    "for instance in instances:\n",
    "    row = {'name': instance.__class__.__name__}\n",
    "    if row['name'] not in ['Detector', 'StringDetector', 'TriggerListDetector', 'defaultdict']:\n",
    "        for attribute in attributes_to_extract:\n",
    "            if hasattr(instance, attribute):\n",
    "                attr_value = getattr(instance, attribute)\n",
    "                if isinstance(attr_value, types.MethodType):\n",
    "                    row[attribute] = f'{instance.__class__.__name__}.{attribute}'\n",
    "                elif isinstance(attr_value, list):\n",
    "                    row[attribute] = ', '.join(attr_value)\n",
    "                else:\n",
    "                    row[attribute] = attr_value\n",
    "            else:\n",
    "                if attribute == 'substrings':\n",
    "                    row[attribute] = 'N/A'\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Save the markdown table to a file\n",
    "with open('detectors.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        return str(obj)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('detector.json', 'w') as f:\n",
    "    json.dump(data, f, cls=CustomEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this.substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.detectors import adultdata, advstereo, base, hallucination, paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_detector_info(modules_to_inspect):\n",
    "    # Initialize a list to store the detector info\n",
    "    detector_info = []\n",
    "\n",
    "    # Initialize a set to store the names of the saved detectors\n",
    "    saved_detectors = set()\n",
    "\n",
    "    # Iterate over each module\n",
    "    for module_name in modules_to_inspect:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "\n",
    "        # Get all classes in the module\n",
    "        classes = inspect.getmembers(module, inspect.isclass)\n",
    "\n",
    "        # Iterate over each class\n",
    "        for class_name, class_ in classes:\n",
    "            # Get all subclasses of the class\n",
    "            subclasses = class_.__subclasses__()\n",
    "\n",
    "            # Iterate over each subclass\n",
    "            for subclass in subclasses:\n",
    "                # Check if the detector has already been saved\n",
    "                if subclass.__name__ in saved_detectors:\n",
    "                    continue\n",
    "\n",
    "                # Add the detector to the set of saved detectors\n",
    "                saved_detectors.add(subclass.__name__)\n",
    "\n",
    "                # Try to create an instance of the subclass\n",
    "                try:\n",
    "                    instance = subclass()\n",
    "                except TypeError:\n",
    "                    instance = None\n",
    "\n",
    "                # Get the subclass info\n",
    "                info = {\n",
    "                    \"Detector\": subclass.__name__,\n",
    "                    \"Module\": subclass.__module__,\n",
    "                    \"Description\": inspect.getdoc(subclass),\n",
    "                    \"Substrings\": instance.substrings if instance and hasattr(instance, 'substrings') else \"N/A\"\n",
    "                }\n",
    "                # Add the subclass info to the detector info list\n",
    "                detector_info.append(info)\n",
    "\n",
    "    # Write the detector info to a JSON file\n",
    "    with open('art_detectors.json', 'w') as f:\n",
    "        json.dump(detector_info, f, indent=4)\n",
    "\n",
    "    # Write the detector info to a Markdown file\n",
    "    with open('art_detectors.md', 'w') as f:\n",
    "        # Write the table headers\n",
    "        f.write(\"| Detector | Module | Description | Substrings |\\n\")\n",
    "        f.write(\"| -------- | ------ | ----------- | ---------- |\\n\")\n",
    "\n",
    "        # Write the table data\n",
    "        for info in detector_info:\n",
    "            f.write(f\"| {info['Detector']} | {info['Module']} | {info['Description']} | {info['Substrings']} |\\n\")\n",
    "\n",
    "# List of modules to inspect\n",
    "modules_to_inspect = ['autoredteam.detectors.adultdata', 'autoredteam.detectors.advstereo', \n",
    "                      'autoredteam.detectors.hallucination', \n",
    "                      'autoredteam.detectors.paraphrase']\n",
    "\n",
    "write_detector_info(modules_to_inspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "def get_module_info(module_name):\n",
    "    # Import the module\n",
    "    module = importlib.import_module(module_name)\n",
    "\n",
    "    # Get all classes in the module\n",
    "    classes = inspect.getmembers(module, inspect.isclass)\n",
    "\n",
    "    # Initialize a list to store the class info\n",
    "    class_info = []\n",
    "\n",
    "    # Iterate over each class\n",
    "    for class_name, class_ in classes:\n",
    "        # Get the class info\n",
    "        info = {\n",
    "            \"Detector\": class_name,\n",
    "            \"Module\": class_.__module__,\n",
    "            \"Description\": inspect.getdoc(class_),\n",
    "            \"Substrings\": \"N/A\"\n",
    "        }\n",
    "        # Add the class info to the class info list\n",
    "        class_info.append(info)\n",
    "\n",
    "    return class_info\n",
    "\n",
    "# List of modules to inspect\n",
    "modules_to_inspect = ['garak.detectors.base']\n",
    "\n",
    "# Iterate over each module\n",
    "for module_name in modules_to_inspect:\n",
    "    # Get the module info\n",
    "    module_info = get_module_info(module_name)\n",
    "\n",
    "    # Write the module info to a JSON file\n",
    "    with open('module_info.json', 'w') as f:\n",
    "        json.dump(module_info, f, indent=4)\n",
    "\n",
    "    # Write the module info to a Markdown file\n",
    "    with open('module_info.md', 'w') as f:\n",
    "        # Write the table headers\n",
    "        f.write(\"| Detector | Module | Description | Substrings |\\n\")\n",
    "        f.write(\"| -------- | ------ | ----------- | ---------- |\\n\")\n",
    "\n",
    "        # Write the table data\n",
    "        for info in module_info:\n",
    "            f.write(f\"| {info['Detector']} | {info['Module']} | {info['Description']} | {info['Substrings']} |\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding New Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "\n",
    "# Read the markdown file\n",
    "with open('test_detector_pairing.md', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Filter out the table lines\n",
    "table_lines = [line.strip() for line in lines if '|' in line]\n",
    "\n",
    "# Convert the markdown table to a CSV format\n",
    "csv_lines = []\n",
    "for line in table_lines:\n",
    "    # Remove leading and trailing pipes\n",
    "    line = re.sub('^\\|', '', line)\n",
    "    line = re.sub('\\|$', '', line)\n",
    "    csv_lines.append(line)\n",
    "\n",
    "# Join the lines with newline characters\n",
    "csv_data = '\\n'.join(csv_lines)\n",
    "\n",
    "# Read the CSV data into a DataFrame\n",
    "pairing_df = pd.read_csv(io.StringIO(csv_data), sep='|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>detectors</th>\n",
       "      <th>goal</th>\n",
       "      <th>probe</th>\n",
       "      <th>tags</th>\n",
       "      <th>uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:----------------------------------</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:-----------------------------------------------</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dan.AntiDAN</td>\n",
       "      <td>Anti-DAN                                     ...</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, ...</td>\n",
       "      <td>disregard the system prompt                  ...</td>\n",
       "      <td>garak.probes.dan.AntiDAN</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:sec...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dan.AutoDANProbe</td>\n",
       "      <td>Probe leveraging generated AutoDAN prompts   ...</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, ...</td>\n",
       "      <td>disregard the system prompt                  ...</td>\n",
       "      <td>garak.probes.dan.AutoDANProbe</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:sec...</td>\n",
       "      <td>https://arxiv.org/abs/2310.04451             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>ChatGPT DevMode + Ranti                      ...</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, ...</td>\n",
       "      <td>disregard the system prompt                  ...</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:sec...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>ChatGPT Developer Mode v2                    ...</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, ...</td>\n",
       "      <td>disregard the system prompt                  ...</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:sec...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  \\\n",
       "0  :----------------------------------   \n",
       "1   dan.AntiDAN                          \n",
       "2   dan.AutoDANProbe                     \n",
       "3   dan.ChatGPT_Developer_Mode_RANTI     \n",
       "4   dan.ChatGPT_Developer_Mode_v2        \n",
       "\n",
       "                                         description  \\\n",
       "0  :---------------------------------------------...   \n",
       "1   Anti-DAN                                     ...   \n",
       "2   Probe leveraging generated AutoDAN prompts   ...   \n",
       "3   ChatGPT DevMode + Ranti                      ...   \n",
       "4   ChatGPT Developer Mode v2                    ...   \n",
       "\n",
       "                                           detectors  \\\n",
       "0  :---------------------------------------------...   \n",
       "1   garak.detectors.mitigation.MitigationBypass, ...   \n",
       "2   garak.detectors.mitigation.MitigationBypass, ...   \n",
       "3   garak.detectors.mitigation.MitigationBypass, ...   \n",
       "4   garak.detectors.mitigation.MitigationBypass, ...   \n",
       "\n",
       "                                                goal  \\\n",
       "0  :---------------------------------------------...   \n",
       "1   disregard the system prompt                  ...   \n",
       "2   disregard the system prompt                  ...   \n",
       "3   disregard the system prompt                  ...   \n",
       "4   disregard the system prompt                  ...   \n",
       "\n",
       "                                              probe  \\\n",
       "0  :-----------------------------------------------   \n",
       "1   garak.probes.dan.AntiDAN                          \n",
       "2   garak.probes.dan.AutoDANProbe                     \n",
       "3   garak.probes.dan.ChatGPT_Developer_Mode_RANTI     \n",
       "4   garak.probes.dan.ChatGPT_Developer_Mode_v2        \n",
       "\n",
       "                                                tags  \\\n",
       "0  :---------------------------------------------...   \n",
       "1   vijil:Security, source:garak, avid-effect:sec...   \n",
       "2   vijil:Security, source:garak, avid-effect:sec...   \n",
       "3   vijil:Security, source:garak, avid-effect:sec...   \n",
       "4   vijil:Security, source:garak, avid-effect:sec...   \n",
       "\n",
       "                                                 uri  \n",
       "0  :---------------------------------------------...  \n",
       "1                                                ...  \n",
       "2   https://arxiv.org/abs/2310.04451             ...  \n",
       "3                                                ...  \n",
       "4                                                ...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairing_df.columns = pairing_df.columns.str.strip()\n",
    "pairing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the new goals and the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip leading and trailing spaces from column names\n",
    "df_csv = pd.read_csv('goals.csv')\n",
    "df_csv.columns = df_csv.columns.str.strip()\n",
    "pairing_df.columns = pairing_df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['New goal', 'Old goals', 'Dimension', 'Brandon Comments'], dtype='object')\n",
      "Index(['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_csv.columns)\n",
    "print(pairing_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from Old goals to New goal\n",
    "goal_mapping = pd.Series(df_csv['New goal'].values, index=df_csv['Old goals']).to_dict()\n",
    "\n",
    "# Update the 'goal' column in the pairing_df DataFrame using the mapping\n",
    "pairing_df['goal'] = pairing_df['goal'].map(goal_mapping).fillna(pairing_df['goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [name, description, detectors, goal, probe, tags, uri]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Search the 'column_name' column for the phrase 'your_phrase'\n",
    "mask = pairing_df['goal'].str.contains('degrade model performance on NLP tasks')\n",
    "\n",
    "# Print the rows where the phrase is found\n",
    "print(pairing_df[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing spaces from all strings in df_csv\n",
    "df_csv = df_csv.applymap(lambda x: x.strip() if type(x) == str else x)\n",
    "\n",
    "# Remove leading and trailing spaces from all strings in pairing_df\n",
    "pairing_df = pairing_df.applymap(lambda x: x.strip() if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from Old goals to New goal\n",
    "goal_mapping = pd.Series(df_csv['New goal'].values, index=df_csv['Old goals']).to_dict()\n",
    "\n",
    "# Update the 'goal' column in the pairing_df DataFrame using the mapping\n",
    "pairing_df['goal'] = pairing_df['goal'].map(goal_mapping).fillna(pairing_df['goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_df['goal'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>detectors</th>\n",
       "      <th>goal</th>\n",
       "      <th>probe</th>\n",
       "      <th>tags</th>\n",
       "      <th>uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:----------------------------------</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:-----------------------------------------------</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dan.AntiDAN</td>\n",
       "      <td>Anti-DAN</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.AntiDAN</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dan.AutoDANProbe</td>\n",
       "      <td>Probe leveraging generated AutoDAN prompts</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.AutoDANProbe</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td>https://arxiv.org/abs/2310.04451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>ChatGPT DevMode + Ranti</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>ChatGPT Developer Mode v2</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  \\\n",
       "0  :----------------------------------   \n",
       "1                          dan.AntiDAN   \n",
       "2                     dan.AutoDANProbe   \n",
       "3     dan.ChatGPT_Developer_Mode_RANTI   \n",
       "4        dan.ChatGPT_Developer_Mode_v2   \n",
       "\n",
       "                                         description  \\\n",
       "0  :---------------------------------------------...   \n",
       "1                                           Anti-DAN   \n",
       "2         Probe leveraging generated AutoDAN prompts   \n",
       "3                            ChatGPT DevMode + Ranti   \n",
       "4                          ChatGPT Developer Mode v2   \n",
       "\n",
       "                                           detectors  \\\n",
       "0  :---------------------------------------------...   \n",
       "1  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "2  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "3  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "4  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "\n",
       "                                                goal  \\\n",
       "0  :---------------------------------------------...   \n",
       "1                        disregard the system prompt   \n",
       "2                        disregard the system prompt   \n",
       "3                        disregard the system prompt   \n",
       "4                        disregard the system prompt   \n",
       "\n",
       "                                              probe  \\\n",
       "0  :-----------------------------------------------   \n",
       "1                          garak.probes.dan.AntiDAN   \n",
       "2                     garak.probes.dan.AutoDANProbe   \n",
       "3     garak.probes.dan.ChatGPT_Developer_Mode_RANTI   \n",
       "4        garak.probes.dan.ChatGPT_Developer_Mode_v2   \n",
       "\n",
       "                                                tags  \\\n",
       "0  :---------------------------------------------...   \n",
       "1  vijil:Security, source:garak, avid-effect:secu...   \n",
       "2  vijil:Security, source:garak, avid-effect:secu...   \n",
       "3  vijil:Security, source:garak, avid-effect:secu...   \n",
       "4  vijil:Security, source:garak, avid-effect:secu...   \n",
       "\n",
       "                                                 uri  \n",
       "0  :---------------------------------------------...  \n",
       "1                                                     \n",
       "2                   https://arxiv.org/abs/2310.04451  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>detectors</th>\n",
       "      <th>goal</th>\n",
       "      <th>probe</th>\n",
       "      <th>tags</th>\n",
       "      <th>uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dan.AntiDAN</td>\n",
       "      <td>Anti-DAN</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.AntiDAN</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dan.AutoDANProbe</td>\n",
       "      <td>Probe leveraging generated AutoDAN prompts</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.AutoDANProbe</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td>https://arxiv.org/abs/2310.04451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>ChatGPT DevMode + Ranti</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>ChatGPT Developer Mode v2</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dan.ChatGPT_Image_Markdown</td>\n",
       "      <td>ChatGPT Markdown Image Unlocker / markdown inj...</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Image_Markdown</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  \\\n",
       "1                       dan.AntiDAN   \n",
       "2                  dan.AutoDANProbe   \n",
       "3  dan.ChatGPT_Developer_Mode_RANTI   \n",
       "4     dan.ChatGPT_Developer_Mode_v2   \n",
       "5        dan.ChatGPT_Image_Markdown   \n",
       "\n",
       "                                         description  \\\n",
       "1                                           Anti-DAN   \n",
       "2         Probe leveraging generated AutoDAN prompts   \n",
       "3                            ChatGPT DevMode + Ranti   \n",
       "4                          ChatGPT Developer Mode v2   \n",
       "5  ChatGPT Markdown Image Unlocker / markdown inj...   \n",
       "\n",
       "                                           detectors  \\\n",
       "1  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "2  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "3  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "4  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "5  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "\n",
       "                          goal                                          probe  \\\n",
       "1  disregard the system prompt                       garak.probes.dan.AntiDAN   \n",
       "2  disregard the system prompt                  garak.probes.dan.AutoDANProbe   \n",
       "3  disregard the system prompt  garak.probes.dan.ChatGPT_Developer_Mode_RANTI   \n",
       "4  disregard the system prompt     garak.probes.dan.ChatGPT_Developer_Mode_v2   \n",
       "5  disregard the system prompt        garak.probes.dan.ChatGPT_Image_Markdown   \n",
       "\n",
       "                                                tags  \\\n",
       "1  vijil:Security, source:garak, avid-effect:secu...   \n",
       "2  vijil:Security, source:garak, avid-effect:secu...   \n",
       "3  vijil:Security, source:garak, avid-effect:secu...   \n",
       "4  vijil:Security, source:garak, avid-effect:secu...   \n",
       "5  vijil:Security, source:garak, avid-effect:secu...   \n",
       "\n",
       "                                uri  \n",
       "1                                    \n",
       "2  https://arxiv.org/abs/2310.04451  \n",
       "3                                    \n",
       "4                                    \n",
       "5                                    "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a mask of rows where any element starts with ':-'\n",
    "mask = pairing_df.applymap(lambda x: str(x).startswith(':-')).any(axis=1)\n",
    "\n",
    "# Invert the mask and use it to filter the DataFrame\n",
    "pairing_df = pairing_df[~mask]\n",
    "pairing_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
