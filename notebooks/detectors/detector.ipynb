{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/Users/vigil/Desktop/Vigil/autoredteam/autoredteam/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def get_detectors(test_detector_dict):\n",
    "    extracted_values = {}\n",
    "    for key, values in test_detector_dict.items():\n",
    "        extracted_values[key] = [f\"{value.__class__.__module__}.{value.__class__.__name__}\" for value in values]\n",
    "    return extracted_values\n",
    "\n",
    "\n",
    "def get_counts(extracted_values):\n",
    "    values = [value for sublist in extracted_values.values() for value in sublist]\n",
    "    value_counts = Counter(values)\n",
    "\n",
    "    return value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OctoAI Agent: mistral-7b-instruct-fp16\n"
     ]
    }
   ],
   "source": [
    "from autoredteam.agents.octo import OctoAPI\n",
    "agent = OctoAPI(name = \"mistral-7b-instruct-fp16\", generations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.harnesses.dimension import SecurityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness\n",
    "from autoredteam.harnesses.owasp import LLMTop10Harness\n",
    "harnesses = [SecurityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness, LLMTop10Harness]\n",
    "harness_instances = [harness(agent) for harness in harnesses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [instance.test_instances for instance in harness_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for each element in sec_tests and store the dictionaries in another list\n",
    "test_detector_dicts = [{test: test.detectors for test in sec_test} for sec_test in tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_list = list(test_detector_dicts[1].keys())\n",
    "tester = keys_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing the JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "# Load JSON file\n",
    "with open('garak_detectors.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "uuid_dict = {}\n",
    "# Assuming data is a list of dictionaries\n",
    "for item in data:\n",
    "    \n",
    "    # Change 'name' field to 'detector_reference' and set its value to the value of 'detectorname'\n",
    "    item['detector_reference'] = item.pop('detectorname')\n",
    "\n",
    "    # Add 'version' field with a string value of \"1.0\"\n",
    "    item['version'] = \"1.0\"\n",
    "\n",
    "    # Change 'description' field to 'detector_logic_description'\n",
    "    item['detector_logic_description'] = item.pop('description')\n",
    "\n",
    "    # Add 'detector_attribute' field with an empty string as its value\n",
    "    item['detector_attribute'] = item['name']\n",
    "\n",
    "    item['language'] = item.pop('bcp47')\n",
    "\n",
    "    # Remove 'name' field\n",
    "    item.pop('name', None)\n",
    "\n",
    "    # Modify 'detector_attribute' field\n",
    "    if 'detector_attribute' in item:\n",
    "        item['detector_attribute'] = item['detector_attribute'].replace(\"<class '\", \"\").replace(\"'>\", \"\")\n",
    "    # Generate a UUID for each dictionary and assign it to the 'id' field\n",
    "    item['id'] = str(uuid.uuid4())\n",
    "\n",
    "# Save data as JSONL file\n",
    "with open('detector_attributes.jsonl', 'w') as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load JSONL file\n",
    "with open('detector_attributes.jsonl', 'r') as f:\n",
    "    jsonl_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Load JSON file\n",
    "with open('art_detectors.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Merge JSON data into JSONL data\n",
    "for json_item in json_data:\n",
    "    new_item = {\n",
    "        'detect': json_item['Detector'] + '.detect',\n",
    "        'tags': \"\",\n",
    "        'uri': \"\",\n",
    "        'substrings': json_item['Substrings'],\n",
    "        'detector_reference': json_item['Module'] + '.' + json_item['Detector'],\n",
    "        'version': \"1.0\",\n",
    "        'detector_logic_description': json_item['Description'],\n",
    "        'detector_attribute': json_item['Module'],\n",
    "        'language': None,\n",
    "        'id': str(uuid.uuid4())\n",
    "    }\n",
    "    jsonl_data.append(new_item)\n",
    "\n",
    "# Save updated data back to JSONL file\n",
    "with open('detector_attributes.jsonl', 'w') as f:\n",
    "    for item in jsonl_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "# Read all lines from the input file\n",
    "with open('probe_attributes.jsonl', 'r') as input_file:\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Open the input file in write mode to overwrite it\n",
    "with open('probe_attributes.jsonl', 'w') as output_file:\n",
    "    # Iterate over each line\n",
    "    for line in lines:\n",
    "        # Load the JSON object\n",
    "        obj = json.loads(line)\n",
    "\n",
    "        # Change \"prompt_id\" to \"prompt_ids\"\n",
    "        obj['prompt_ids'] = obj.pop('prompt_id')\n",
    "\n",
    "        # Change \"id\" to \"name\" and remove the prefix before the dot\n",
    "        obj['name'] = obj.pop('id').split('.')[-1]\n",
    "\n",
    "        # Add a new \"id\" field with a unique UUID\n",
    "        obj['id'] = str(uuid.uuid4())\n",
    "\n",
    "        # Write the modified JSON object to the output file\n",
    "        output_file.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order of the fields\n",
    "field_order = [\"id\", \"name\", \"description\", \"version\", \"goal\", \"tags\", \"prompt_ids\", \"detectors\"]\n",
    "\n",
    "# Read all lines from the input file\n",
    "with open('probe_attributes.jsonl', 'r') as input_file:\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Open the output file\n",
    "with open('reorganized_probe_attributes.jsonl', 'w') as output_file:\n",
    "    # Iterate over each line\n",
    "    for line in lines:\n",
    "        # Load the JSON object\n",
    "        obj = json.loads(line)\n",
    "\n",
    "        # Reorganize the fields\n",
    "        obj = {field: obj.get(field, None) for field in field_order + [f for f in obj.keys() if f not in field_order]}\n",
    "\n",
    "        # Write the modified JSON object to the output file\n",
    "        output_file.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Test Information\n",
    "Below is for getting the test information. It is saved in three different formats and pulls everything from the harnesses above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv \n",
    "\n",
    "def get_object_attributes(obj):\n",
    "    # Get all attributes of the object\n",
    "    attributes = dir(obj)\n",
    "    # Filter out private and special attributes\n",
    "    user_defined_attributes = {attr: getattr(obj, attr) for attr in attributes if not attr.startswith('__')}\n",
    "    # Exclude methods and convert non-serializable objects\n",
    "    user_defined_attributes = {attr: convert_to_serializable(attr, value) for attr, value in user_defined_attributes.items() if not callable(value)}\n",
    "    return user_defined_attributes\n",
    "\n",
    "def convert_to_serializable(attr, value):\n",
    "    if isinstance(value, (int, float, str, bool, type(None))):\n",
    "        # If the value is already JSON serializable, return it as is\n",
    "        return value\n",
    "    elif hasattr(value, '__dict__'):\n",
    "        # If the value has a '__dict__' attribute, assume it's an object and convert it to a string\n",
    "        return str(value)\n",
    "    elif isinstance(value, list) and attr in ['detectors', 'probe']:\n",
    "        # If the attribute is \"detectors\" or \"probe\" and the value is a list of objects, convert it to a string containing class names\n",
    "        class_names = [type(item).__module__ + '.' + type(item).__name__ for item in value]\n",
    "        return ', '.join(class_names)\n",
    "    else:\n",
    "        # If all else fails, return a string representation of the value\n",
    "        return repr(value)\n",
    "\n",
    "\n",
    "def save_object_attributes(obj_lists, filename_prefix):\n",
    "    all_attributes = []\n",
    "    # Collect all unique keys\n",
    "    all_keys = set()\n",
    "    for i, obj_list in enumerate(obj_lists, start=1):\n",
    "        for j, obj in enumerate(obj_list, start=1):\n",
    "            # Get the object's attributes\n",
    "            obj_attributes = get_object_attributes(obj)\n",
    "            # Append to the list of all attributes\n",
    "            all_attributes.append(obj_attributes)\n",
    "            # Update the set of keys\n",
    "            all_keys.update(obj_attributes.keys())\n",
    "\n",
    "\n",
    "    # Save to JSON\n",
    "    json_filename = f\"{filename_prefix}.json\"\n",
    "    with open(json_filename, 'w') as file:\n",
    "        json.dump(all_attributes, file, indent=4)\n",
    "    \n",
    "    # Save to Markdown\n",
    "    md_filename = f\"{filename_prefix}.md\"\n",
    "    with open(md_filename, 'w') as file:\n",
    "        # Write table header\n",
    "        file.write(\"|\")\n",
    "        for key in all_keys:\n",
    "            file.write(f\"{key}|\")\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"|\")\n",
    "        for _ in all_keys:\n",
    "            file.write(\"---|\")\n",
    "        file.write(\"\\n\")\n",
    "        # Write table rows\n",
    "        for attributes in all_attributes:\n",
    "            file.write(\"|\")\n",
    "            for key in all_keys:\n",
    "                file.write(f\"{attributes.get(key, '')}|\")\n",
    "            file.write(\"\\n\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_filename = f\"{filename_prefix}.csv\"\n",
    "    with open(csv_filename, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=all_keys)\n",
    "        writer.writeheader()\n",
    "        for attributes in all_attributes:\n",
    "            writer.writerow(attributes)\n",
    "\n",
    "\n",
    "\n",
    "# Save attributes of all tests to separate JSON files\n",
    "save_object_attributes(tests, 'test_attributes')\n",
    "\n",
    "# Alternatively, you can save attributes of all tests to a single JSON file\n",
    "# save_test_attributes(tests_list, 'all_tests_attributes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating with new goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Load the mapping CSV file\n",
    "df_csv = pd.read_csv('goals.csv')\n",
    "df_csv.columns = df_csv.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from old values to new ones\n",
    "mapping = pd.Series(df_csv['New goal'].values, index=df_csv['Old goals']).to_dict()\n",
    "# Replace values in the CSV file\n",
    "df = pd.read_csv('test_attributes.csv')\n",
    "df.replace({'goal': mapping}, inplace=True)\n",
    "df.to_csv('test_attributes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in the JSON file\n",
    "with open('test_attributes.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = [{k: mapping.get(v, v) if k == 'goal' else v for k, v in item.items()} for item in data]\n",
    "\n",
    "with open('test_attributes.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in the Markdown file\n",
    "with open('test_attributes.md', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [line.replace(old, str(mapping[old])) for line in lines for old in mapping if old in line]\n",
    "\n",
    "with open('test_attributes.md', 'w') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding prompt ID from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.engine.dataloaders import DBGetter\n",
    "import sys\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "load_dotenv(\"/Users/vigil/Desktop/Vigil/autoredteam/autoredteam/.env\")\n",
    "\n",
    "probe = \"<garak.probes.xss.MarkdownImageExfil object at 0x387a51b80>\"\n",
    "def probe_id_extraction(probe):\n",
    "    def extract_parts(input_str):\n",
    "        match = re.search(r'<(garak)\\.probes\\.(\\w+)\\.(\\w+)', input_str)\n",
    "        if match:\n",
    "            return f\"{match.group(1)}.{match.group(2)}\", match.group(3)\n",
    "        return None, None\n",
    "\n",
    "    source, probe_name = extract_parts(probe)\n",
    "    db_getter = DBGetter(SOURCE_STB=source, env_path=\"/Users/vigil/Desktop/Vigil/autoredteam/autoredteam/.env\")\n",
    "    prompt_id = db_getter.get_by_key(probe_name=probe_name, key=\"_id\")\n",
    "    return prompt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UUID('09c32e8a-aab0-9818-5074-f2bf7c7d1aa2'),\n",
       " UUID('663b30fa-ad4b-8d4f-afb0-fc0d2f3216d9'),\n",
       " UUID('918f8eb6-b42f-f7fe-2f04-cb0b3f8159ba'),\n",
       " UUID('f51b9348-c8b4-4c62-5929-8b821709923c'),\n",
       " UUID('768462f3-00dc-9e49-b5b8-0ecff8514939'),\n",
       " UUID('3be591ec-91fe-1b5b-ae31-75b685fb40a3'),\n",
       " UUID('e29a1213-0f12-6e07-8728-5c45ba45bf11'),\n",
       " UUID('ba915ef7-d1c7-c8e7-097a-5b3dc4ca28a5'),\n",
       " UUID('e053cbf5-d737-f448-f79a-c0ebe6a0f1dc'),\n",
       " UUID('a0395cbf-f9ab-eab5-91a4-277ba722ed80'),\n",
       " UUID('c1e9e928-3769-815d-6d4e-1b98734b81f5'),\n",
       " UUID('26c8689c-2f56-1281-b324-f1b5ad4afbfb')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe_id_extraction(probe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Probe ID Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:   0%|          | 0/133 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 133/133 [03:24<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from autoredteam.engine.dataloaders import DBGetter\n",
    "from uuid import UUID\n",
    "from tqdm import tqdm\n",
    "\n",
    "def probe_id_extraction(probe):\n",
    "    def extract_parts(input_str):\n",
    "        match = re.search(r'<(garak)\\.probes\\.(\\w+)\\.(\\w+)', input_str)\n",
    "        if match:\n",
    "            return f\"{match.group(1)}.{match.group(2)}\", match.group(3)\n",
    "        return None, None\n",
    "\n",
    "    source, probe_name = extract_parts(probe)\n",
    "    db_getter = DBGetter(SOURCE_STB=source, env_path=\"/Users/vigil/Desktop/Vigil/autoredteam/autoredteam/.env\")\n",
    "    prompt_id = db_getter.get_by_key(probe_name=probe_name, key=\"_id\")\n",
    "    return str(prompt_id)  # Convert UUID to string\n",
    "\n",
    "# Load JSON file\n",
    "with open('test_attributes.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Apply probe_id_extraction() to each 'probe' field and store the result in a new 'prompt_id' field\n",
    "for item in tqdm(data, desc=\"Processing items\"):\n",
    "    item['prompt_id'] = probe_id_extraction(item['probe'])\n",
    "\n",
    "# Save updated JSON data back to file\n",
    "with open('file.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON file\n",
    "with open('file.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Assuming data is a list of dictionaries\n",
    "for item in data:\n",
    "    item.pop('prompts', None)\n",
    "    item.pop('active', None)\n",
    "    item.pop('logged', None)\n",
    "    item['id'] = item.pop('name')\n",
    "    item['tags'] = item['tags'].strip(\"[]\").split(\", \")\n",
    "    item['prompt_id'] = [item['prompt_id'].strip(\"[]\")]\n",
    "    item['goal'] = [item['goal']]\n",
    "    \n",
    "     # Extract UUIDs and put them back as separate strings\n",
    "    if 'prompt_id' in item:\n",
    "        uuids = re.findall(r\"UUID\\('(.*?)'\\)\", item['prompt_id'][0])\n",
    "        item['prompt_id'] = [\"UUID('\" + uuid + \"')\" for uuid in uuids]\n",
    "\n",
    "\n",
    "\n",
    "    match = re.search(r'<(.*?) object', item['probe'])\n",
    "    if match:\n",
    "        item['probe'] = match.group(1)\n",
    "    # Split 'detectors' field into a list of strings\n",
    "    item['detectors'] = [x.strip() for x in item['detectors'].split(',')]\n",
    "    # Add 'version' field with a float value of 1.0\n",
    "    item['version'] = \"1.0\"\n",
    "\n",
    "# Save data as JSONL file\n",
    "with open('probe_attributes.jsonl', 'w') as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloze data replay for literature. Uses passages that either GPT-4 or ChatGPT have been known to replay. - based on articles from The Guardian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<garak.detectors.base.TriggerListDetector at 0x317ef8f20>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tester.description)\n",
    "tester.detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib.util\n",
    "import inspect\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/vigil/Desktop/Vigil/autoredteam')\n",
    "\n",
    "# Define the directory with the .py files\n",
    "directory = '/Users/vigil/Desktop/Vigil/autoredteam/autoredteam/tests'\n",
    "\n",
    "# Define the markdown file\n",
    "markdown_directory = 'markdown_files'\n",
    "markdown_file = os.path.join(markdown_directory, 'attributes.md')\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(markdown_directory, exist_ok=True)\n",
    "# Open the markdown file\n",
    "with open(markdown_file, 'w') as f:\n",
    "    # List all .py files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.py'):\n",
    "            # Import the .py file as a module\n",
    "            spec = importlib.util.spec_from_file_location(filename[:-3], os.path.join(directory, filename))\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "\n",
    "            # Get the attributes of the module\n",
    "            attributes = inspect.getmembers(module)\n",
    "\n",
    "            # Filter the attributes\n",
    "            attributes = [attr for attr in attributes if attr[0] in ['__doc__', 'detectors', 'uri', 'name', 'description', 'tags', 'goal']]\n",
    "\n",
    "            # Write the attributes to the markdown file in a table format\n",
    "            f.write(f'## {filename[:-3]}\\n')\n",
    "            f.write('| Attribute | Value |\\n')\n",
    "            f.write('| --- | --- |\\n')\n",
    "            for attr in attributes:\n",
    "                f.write(f'| {attr[0]} | {attr[1]} |\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the markdown file\n",
    "markdown_file = 'markdown_files/all_tests.md'\n",
    "\n",
    "# Create a directory for the markdown file\n",
    "os.makedirs(os.path.dirname(markdown_file), exist_ok=True)\n",
    "\n",
    "# Create a dictionary to group the subtests by their categories\n",
    "tests_by_category = defaultdict(list)\n",
    "\n",
    "# Iterate over all dictionaries in test_detector_dicts\n",
    "for test_detector_dict in test_detector_dicts:\n",
    "    # Iterate over the items in each dictionary\n",
    "    for test, detectors in test_detector_dict.items():\n",
    "        # Extract the relevant part of the test name\n",
    "        test_category = str(test).split('.')[2]\n",
    "        test_name = str(test).split(' ')[0].split('.')[-1]\n",
    "\n",
    "        # Group the subtests by their categories\n",
    "        tests_by_category[test_category].append((test_name, detectors))\n",
    "\n",
    "# Open the markdown file\n",
    "with open(markdown_file, 'w') as f:\n",
    "    # Iterate over the categories\n",
    "    for category, tests in tests_by_category.items():\n",
    "        # Write the category to the file\n",
    "        f.write(f'## {category}\\n')\n",
    "\n",
    "        # Write the subtests to the file\n",
    "        for test_name, detectors in tests:\n",
    "            f.write(f'- {test_name}\\n')\n",
    "\n",
    "            # Write the detectors to the file\n",
    "            for detector in detectors:\n",
    "                # Extract the relevant part of the detector name\n",
    "                detector_name = '.'.join(str(detector).split(' ')[0].split('.')[1:])\n",
    "                f.write(f'  - {detector_name}\\n')\n",
    "\n",
    "        # Add a newline for readability\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update each dictionary with the result of calling get_detectors on it\n",
    "for i in range(len(test_detector_dicts)):\n",
    "    test_detector_dicts[i] = get_detectors(test_detector_dicts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detector_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the markdown file\n",
    "markdown_file = 'markdown_files/all_tests.md'\n",
    "\n",
    "# Create a directory for the markdown file\n",
    "os.makedirs(os.path.dirname(markdown_file), exist_ok=True)\n",
    "\n",
    "# Open the markdown file\n",
    "with open(markdown_file, 'w') as f:\n",
    "    # Iterate over the dictionary in test_detector_dicts\n",
    "    for test, detectors in test_detector_dicts[0].items():\n",
    "        # Extract the test name from the object string\n",
    "        test_name = str(test).split(' ')[0].split('.')[-1]\n",
    "\n",
    "        # Write the test name to the file\n",
    "        f.write(f'## {test_name}\\n')\n",
    "\n",
    "        # Write the detectors to the file\n",
    "        for detector in detectors:\n",
    "            f.write(f'- {detector}\\n')\n",
    "\n",
    "        # Add a newline for readability\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counts = [get_counts(dict) for dict in test_detector_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness_detector_counts= list(zip(harnesses, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness_detector_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Iterate over the zipped list\n",
    "for harness, counter in harness_detector_counts:\n",
    "    # Get the names of the detectors and their counts\n",
    "    detectors = list(counter.keys())\n",
    "    counts = list(counter.values())\n",
    "    \n",
    "    # Sort the detectors and counts by the counts\n",
    "    detectors, counts = zip(*sorted(zip(detectors, counts), key=lambda x: x[1]))\n",
    "    \n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(10, 5))  # Set the figure size\n",
    "    plt.barh(detectors, counts)  # Create a horizontal bar chart\n",
    "    plt.title(str(harness))  # Set the title to the name of the harness\n",
    "    plt.xlabel('Counts')  # Set the x-axis label\n",
    "    plt.ylabel('Detectors')  # Set the y-axis label\n",
    "    plt.xlim(0, max(counts) + 1)  # Set the x-axis limits\n",
    "    plt.xticks(np.arange(0, max(counts) + 1, step=1))  # Set the x-axis ticks to integers\n",
    "    plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.harnesses.dimension import * \n",
    "harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "harness_instances = [harness(agent) for harness in harnesses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.detectors.base import *\n",
    "detectors = [Detector, HFDetector, StringAbsenceDetector, StringDetector, TriggerListAbsenceDetector, TriggerListDetector]\n",
    "first_instance = detectors[0]()\n",
    "\n",
    "# Get all the attributes and methods of the instance\n",
    "attributes = dir(first_instance)\n",
    "\n",
    "# Print the attributes\n",
    "for attribute in attributes:\n",
    "    print(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.detectors.base import *\n",
    "detectors = [Detector, StringDetector, TriggerListAbsenceDetector, TriggerListDetector]\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['bcp47', 'description', 'detect', 'detectorname', 'name', 'precision', 'recall', 'tags', 'uri']\n",
    "\n",
    "for DetectorClass in detectors:\n",
    "    # Create an instance of the class\n",
    "    instance = DetectorClass()\n",
    "\n",
    "    # Extract the specified attributes\n",
    "    for attribute in attributes_to_extract:\n",
    "        if hasattr(instance, attribute):\n",
    "            print(f'{attribute}: {getattr(instance, attribute)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "# List of attributes to look for\n",
    "attributes_to_look_for = ['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri']\n",
    "\n",
    "instances_data = []\n",
    "for instance in test.test_instances:\n",
    "    instance_dict = {}\n",
    "    for attribute in attributes_to_look_for:\n",
    "        if hasattr(instance, attribute):\n",
    "            if attribute == 'detectors':\n",
    "                # Get the class name of each detector and join them into a single string\n",
    "                instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "            elif attribute == 'probe':\n",
    "                # Get the class name of the probe\n",
    "                probe = getattr(instance, attribute)\n",
    "                instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "            elif attribute == 'tags':\n",
    "                # Join the tags into a single string\n",
    "                instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "            else:\n",
    "                instance_dict[attribute] = getattr(instance, attribute)\n",
    "    instances_data.append(instance_dict)\n",
    "\n",
    "print(instances_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Add a title to the markdown\n",
    "markdown_table = '# Security\\n' + markdown_table\n",
    "\n",
    "print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.harnesses.dimension import * \n",
    "from tabulate import tabulate\n",
    "\n",
    "harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "harness_instances = [harness(agent) for harness in harnesses]\n",
    "\n",
    "# List of attributes to look for\n",
    "attributes_to_look_for = ['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri']\n",
    "\n",
    "for harness_instance in harness_instances:\n",
    "    instances_data = []\n",
    "    for instance in harness_instance.test_instances:\n",
    "        instance_dict = {}\n",
    "        for attribute in attributes_to_look_for:\n",
    "            if hasattr(instance, attribute):\n",
    "                if attribute == 'detectors':\n",
    "                    # Get the class name of each detector and join them into a single string\n",
    "                    instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "                elif attribute == 'probe':\n",
    "                    # Get the class name of the probe\n",
    "                    probe = getattr(instance, attribute)\n",
    "                    instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "                elif attribute == 'tags':\n",
    "                    # Join the tags into a single string\n",
    "                    instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "                elif attribute == 'description':\n",
    "                    # Check if the description is a tuple, and if so, take the first element\n",
    "                    description = getattr(instance, attribute)\n",
    "                    if isinstance(description, tuple):\n",
    "                        description = description[0]\n",
    "                    instance_dict[attribute] = description\n",
    "                else:\n",
    "                    instance_dict[attribute] = getattr(instance, attribute)\n",
    "        instances_data.append(instance_dict)\n",
    "\n",
    "    # Convert the list of dictionaries to a markdown table\n",
    "    markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "    # Add a title to the markdown\n",
    "    markdown_table = '# ' + type(harness_instance).__name__ + '\\n' + markdown_table\n",
    "\n",
    "    print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.harnesses.dimension import * \n",
    "from tabulate import tabulate\n",
    "\n",
    "def generate_documentation(attributes_to_look_for, save_path):\n",
    "    harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "    harness_instances = [harness(agent) for harness in harnesses]\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        for harness_instance in harness_instances:\n",
    "            instances_data = []\n",
    "            for instance in harness_instance.test_instances:\n",
    "                instance_dict = {}\n",
    "                for attribute in attributes_to_look_for:\n",
    "                    if hasattr(instance, attribute):\n",
    "                        if attribute == 'detectors':\n",
    "                            # Get the class name of each detector and join them into a single string\n",
    "                            instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "                        elif attribute == 'probe':\n",
    "                            # Get the class name of the probe\n",
    "                            probe = getattr(instance, attribute)\n",
    "                            instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "                        elif attribute == 'tags':\n",
    "                            # Join the tags into a single string\n",
    "                            instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "                        else:\n",
    "                            instance_dict[attribute] = getattr(instance, attribute)\n",
    "                instances_data.append(instance_dict)\n",
    "\n",
    "            # Convert the list of dictionaries to a markdown table\n",
    "            markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "            # Add a title to the markdown\n",
    "            markdown_table = '# ' + type(harness_instance).__name__ + '\\n' + markdown_table\n",
    "\n",
    "            # Write the markdown table to the file\n",
    "            f.write(markdown_table + '\\n\\n')\n",
    "\n",
    "# Call the function with the attributes to look for and the save path\n",
    "generate_documentation(['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri'], 'vijil_detectors.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from autoredteam.harnesses.dimension import * \n",
    "from tabulate import tabulate\n",
    "\n",
    "def generate_documentation(attributes_to_look_for, save_path):\n",
    "    harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "    harness_instances = [harness(agent) for harness in harnesses]\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        for harness_instance in harness_instances:\n",
    "            instances_data = []\n",
    "            for instance in harness_instance.test_instances:\n",
    "                instance_dict = {}\n",
    "                for attribute in attributes_to_look_for:\n",
    "                    if hasattr(instance, attribute):\n",
    "                        if attribute == 'detectors':\n",
    "                            # Get the class name of each detector and join them into a single string\n",
    "                            instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "                        elif attribute == 'probe':\n",
    "                            # Get the class name of the probe\n",
    "                            probe = getattr(instance, attribute)\n",
    "                            instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "                        elif attribute == 'tags':\n",
    "                            # Join the tags into a single string\n",
    "                            instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "                        elif attribute == 'description':\n",
    "                            # Check if the description is a tuple, and if so, take the first element\n",
    "                            description = getattr(instance, attribute)\n",
    "                            if isinstance(description, tuple):\n",
    "                                description = description[0]\n",
    "                            instance_dict[attribute] = description\n",
    "                        else:\n",
    "                            instance_dict[attribute] = getattr(instance, attribute)\n",
    "                instances_data.append(instance_dict)\n",
    "\n",
    "            # Convert the list of dictionaries to a markdown table\n",
    "            markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "            # Add a title to the markdown\n",
    "            markdown_table = '# ' + type(harness_instance).__name__ + '\\n' + markdown_table\n",
    "\n",
    "            # Write the markdown table to the file\n",
    "            f.write(markdown_table + '\\n\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Generate documentation for test instances.')\n",
    "    parser.add_argument('attributes', type=str, nargs='+', help='List of attributes to generate documentation for.')\n",
    "    parser.add_argument('save_path', type=str, help='Path to save the markdown file.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Call the function with the attributes to look for and the save path\n",
    "    generate_documentation(args.attributes, args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from garak.detectors.dan import DAN, DANJailbreak, DevMode, DUDE, STAN, AntiDAN, MarkdownLink\n",
    "from tabulate import tabulate\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['bcp47', 'description', 'detect', 'detectorname', 'name', 'precision', 'recall', 'tags', 'uri']\n",
    "\n",
    "instances = [DAN, DANJailbreak, DevMode, DUDE, STAN, AntiDAN, MarkdownLink]\n",
    "\n",
    "data = []\n",
    "\n",
    "# Extract the specified attributes\n",
    "for instance in instances:\n",
    "    row = {'name': instance.__name__}\n",
    "    for attribute in attributes_to_extract:\n",
    "        if hasattr(instance, attribute):\n",
    "            row[attribute] = getattr(instance, attribute)\n",
    "    data.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Save the markdown table to a file\n",
    "with open('output.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import garak.detectors as detectors\n",
    "from tabulate import tabulate\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['detectorname', 'name', 'bcp47', 'description', 'detect', 'tags', 'uri']\n",
    "\n",
    "# Get all modules in the garak.detectors package\n",
    "detector_modules = list(pkgutil.iter_modules(detectors.__path__))\n",
    "\n",
    "instances = []\n",
    "\n",
    "# Import each module and add its classes to the instances list\n",
    "for _, module_name, _ in detector_modules:\n",
    "    module = __import__(f'garak.detectors.{module_name}', fromlist=[module_name])\n",
    "    for attribute_name in dir(module):\n",
    "        attribute = getattr(module, attribute_name)\n",
    "        if isinstance(attribute, type) and attribute_name != 'Attempt':\n",
    "            instances.append(attribute)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Extract the specified attributes\n",
    "for instance in instances:\n",
    "    row = {'name': instance.__name__}\n",
    "    for attribute in attributes_to_extract:\n",
    "        if hasattr(instance, attribute):\n",
    "            row[attribute] = getattr(instance, attribute)\n",
    "    data.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Save the markdown table to a file\n",
    "with open('new_output.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from garak.detectors.riskywords import SurgeProfanityAnimalReferences\n",
    "this = SurgeProfanityAnimalReferences()\n",
    "this.substrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Detector Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import garak.detectors as detectors\n",
    "from tabulate import tabulate\n",
    "import types\n",
    "import json\n",
    "\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, type):\n",
    "            return str(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['detectorname', 'name', 'bcp47', 'description', 'detect', 'tags', 'uri', 'substrings']\n",
    "\n",
    "# Get all modules in the garak.detectors package\n",
    "detector_modules = list(pkgutil.iter_modules(detectors.__path__))\n",
    "\n",
    "instances = []\n",
    "\n",
    "# Import each module and add its classes to the instances list\n",
    "for _, module_name, _ in detector_modules:\n",
    "    module = __import__(f'garak.detectors.{module_name}', fromlist=[module_name])\n",
    "    for attribute_name in dir(module):\n",
    "        attribute = getattr(module, attribute_name)\n",
    "        if isinstance(attribute, type) and attribute_name != 'Attempt':\n",
    "            try:\n",
    "                instances.append(attribute())\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "data = []\n",
    "\n",
    "# Extract the specified attributes\n",
    "for instance in instances:\n",
    "    row = {'name': instance.__class__.__name__}\n",
    "    if row['name'] not in ['Detector', 'StringDetector', 'TriggerListDetector', 'defaultdict']:\n",
    "        for attribute in attributes_to_extract:\n",
    "            if hasattr(instance, attribute):\n",
    "                attr_value = getattr(instance, attribute)\n",
    "                if isinstance(attr_value, types.MethodType):\n",
    "                    row[attribute] = f'{instance.__class__.__name__}.{attribute}'\n",
    "                elif isinstance(attr_value, list):\n",
    "                    row[attribute] = ', '.join(attr_value)\n",
    "                else:\n",
    "                    row[attribute] = attr_value\n",
    "            else:\n",
    "                if attribute == 'substrings':\n",
    "                    row[attribute] = 'N/A'\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Save the markdown table to a file\n",
    "with open('detectors.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        return str(obj)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('detector.json', 'w') as f:\n",
    "    json.dump(data, f, cls=CustomEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this.substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.detectors import adultdata, advstereo, base, hallucination, paraphrase, privateinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "def write_detector_info(modules_to_inspect):\n",
    "    # Initialize a list to store the detector info\n",
    "    detector_info = []\n",
    "\n",
    "    # Initialize a set to store the names of the saved detectors\n",
    "    saved_detectors = set()\n",
    "\n",
    "    # Iterate over each module\n",
    "    for module_name in modules_to_inspect:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "\n",
    "        # Get all classes in the module\n",
    "        classes = inspect.getmembers(module, inspect.isclass)\n",
    "\n",
    "        # Iterate over each class\n",
    "        for class_name, class_ in classes:\n",
    "            # Get all subclasses of the class\n",
    "            subclasses = class_.__subclasses__()\n",
    "\n",
    "            # Iterate over each subclass\n",
    "            for subclass in subclasses:\n",
    "                # Check if the detector has already been saved\n",
    "                if subclass.__name__ in saved_detectors:\n",
    "                    continue\n",
    "\n",
    "                # Add the detector to the set of saved detectors\n",
    "                saved_detectors.add(subclass.__name__)\n",
    "\n",
    "                # Try to create an instance of the subclass\n",
    "                try:\n",
    "                    instance = subclass()\n",
    "                except TypeError:\n",
    "                    instance = None\n",
    "\n",
    "                # Get the subclass info\n",
    "                info = {\n",
    "                    \"Detector\": subclass.__name__,\n",
    "                    \"Module\": subclass.__module__,\n",
    "                    \"Description\": inspect.getdoc(subclass),\n",
    "                    \"Substrings\": instance.substrings if instance and hasattr(instance, 'substrings') else \"N/A\"\n",
    "                }\n",
    "                # Add the subclass info to the detector info list\n",
    "                detector_info.append(info)\n",
    "\n",
    "    # Write the detector info to a JSON file\n",
    "    with open('art_detectors.json', 'w') as f:\n",
    "        json.dump(detector_info, f, indent=4)\n",
    "\n",
    "# List of modules to inspect\n",
    "modules_to_inspect = ['autoredteam.detectors.adultdata', 'autoredteam.detectors.advstereo', \n",
    "                      'autoredteam.detectors.hallucination', \n",
    "                      'autoredteam.detectors.paraphrase', 'autoredteam.detectors.privateinfo']\n",
    "\n",
    "write_detector_info(modules_to_inspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "def get_module_info(module_name):\n",
    "    # Import the module\n",
    "    module = importlib.import_module(module_name)\n",
    "\n",
    "    # Get all classes in the module\n",
    "    classes = inspect.getmembers(module, inspect.isclass)\n",
    "\n",
    "    # Initialize a list to store the class info\n",
    "    class_info = []\n",
    "\n",
    "    # Iterate over each class\n",
    "    for class_name, class_ in classes:\n",
    "        # Get the class info\n",
    "        info = {\n",
    "            \"Detector\": class_name,\n",
    "            \"Module\": class_.__module__,\n",
    "            \"Description\": inspect.getdoc(class_),\n",
    "            \"Substrings\": \"N/A\"\n",
    "        }\n",
    "        # Add the class info to the class info list\n",
    "        class_info.append(info)\n",
    "\n",
    "    return class_info\n",
    "\n",
    "# List of modules to inspect\n",
    "modules_to_inspect = ['garak.detectors.base']\n",
    "\n",
    "# Iterate over each module\n",
    "for module_name in modules_to_inspect:\n",
    "    # Get the module info\n",
    "    module_info = get_module_info(module_name)\n",
    "\n",
    "    # Write the module info to a JSON file\n",
    "    with open('module_info.json', 'w') as f:\n",
    "        json.dump(module_info, f, indent=4)\n",
    "\n",
    "    # Write the module info to a Markdown file\n",
    "    with open('module_info.md', 'w') as f:\n",
    "        # Write the table headers\n",
    "        f.write(\"| Detector | Module | Description | Substrings |\\n\")\n",
    "        f.write(\"| -------- | ------ | ----------- | ---------- |\\n\")\n",
    "\n",
    "        # Write the table data\n",
    "        for info in module_info:\n",
    "            f.write(f\"| {info['Detector']} | {info['Module']} | {info['Description']} | {info['Substrings']} |\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding New Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "\n",
    "# Read the markdown file\n",
    "with open('test_detector_pairing.md', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Filter out the table lines\n",
    "table_lines = [line.strip() for line in lines if '|' in line]\n",
    "\n",
    "# Convert the markdown table to a CSV format\n",
    "csv_lines = []\n",
    "for line in table_lines:\n",
    "    # Remove leading and trailing pipes\n",
    "    line = re.sub('^\\|', '', line)\n",
    "    line = re.sub('\\|$', '', line)\n",
    "    csv_lines.append(line)\n",
    "\n",
    "# Join the lines with newline characters\n",
    "csv_data = '\\n'.join(csv_lines)\n",
    "\n",
    "# Read the CSV data into a DataFrame\n",
    "pairing_df = pd.read_csv(io.StringIO(csv_data), sep='|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>detectors</th>\n",
       "      <th>goal</th>\n",
       "      <th>probe</th>\n",
       "      <th>tags</th>\n",
       "      <th>uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:----------------------------------</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:-----------------------------------------------</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dan.AntiDAN</td>\n",
       "      <td>Anti-DAN                                     ...</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, ...</td>\n",
       "      <td>disregard the system prompt                  ...</td>\n",
       "      <td>garak.probes.dan.AntiDAN</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:sec...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dan.AutoDANProbe</td>\n",
       "      <td>Probe leveraging generated AutoDAN prompts   ...</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, ...</td>\n",
       "      <td>disregard the system prompt                  ...</td>\n",
       "      <td>garak.probes.dan.AutoDANProbe</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:sec...</td>\n",
       "      <td>https://arxiv.org/abs/2310.04451             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>ChatGPT DevMode + Ranti                      ...</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, ...</td>\n",
       "      <td>disregard the system prompt                  ...</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:sec...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>ChatGPT Developer Mode v2                    ...</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, ...</td>\n",
       "      <td>disregard the system prompt                  ...</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:sec...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  \\\n",
       "0  :----------------------------------   \n",
       "1   dan.AntiDAN                          \n",
       "2   dan.AutoDANProbe                     \n",
       "3   dan.ChatGPT_Developer_Mode_RANTI     \n",
       "4   dan.ChatGPT_Developer_Mode_v2        \n",
       "\n",
       "                                         description  \\\n",
       "0  :---------------------------------------------...   \n",
       "1   Anti-DAN                                     ...   \n",
       "2   Probe leveraging generated AutoDAN prompts   ...   \n",
       "3   ChatGPT DevMode + Ranti                      ...   \n",
       "4   ChatGPT Developer Mode v2                    ...   \n",
       "\n",
       "                                           detectors  \\\n",
       "0  :---------------------------------------------...   \n",
       "1   garak.detectors.mitigation.MitigationBypass, ...   \n",
       "2   garak.detectors.mitigation.MitigationBypass, ...   \n",
       "3   garak.detectors.mitigation.MitigationBypass, ...   \n",
       "4   garak.detectors.mitigation.MitigationBypass, ...   \n",
       "\n",
       "                                                goal  \\\n",
       "0  :---------------------------------------------...   \n",
       "1   disregard the system prompt                  ...   \n",
       "2   disregard the system prompt                  ...   \n",
       "3   disregard the system prompt                  ...   \n",
       "4   disregard the system prompt                  ...   \n",
       "\n",
       "                                              probe  \\\n",
       "0  :-----------------------------------------------   \n",
       "1   garak.probes.dan.AntiDAN                          \n",
       "2   garak.probes.dan.AutoDANProbe                     \n",
       "3   garak.probes.dan.ChatGPT_Developer_Mode_RANTI     \n",
       "4   garak.probes.dan.ChatGPT_Developer_Mode_v2        \n",
       "\n",
       "                                                tags  \\\n",
       "0  :---------------------------------------------...   \n",
       "1   vijil:Security, source:garak, avid-effect:sec...   \n",
       "2   vijil:Security, source:garak, avid-effect:sec...   \n",
       "3   vijil:Security, source:garak, avid-effect:sec...   \n",
       "4   vijil:Security, source:garak, avid-effect:sec...   \n",
       "\n",
       "                                                 uri  \n",
       "0  :---------------------------------------------...  \n",
       "1                                                ...  \n",
       "2   https://arxiv.org/abs/2310.04451             ...  \n",
       "3                                                ...  \n",
       "4                                                ...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairing_df.columns = pairing_df.columns.str.strip()\n",
    "pairing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the new goals and the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip leading and trailing spaces from column names\n",
    "df_csv = pd.read_csv('goals.csv')\n",
    "df_csv.columns = df_csv.columns.str.strip()\n",
    "pairing_df.columns = pairing_df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['New goal', 'Old goals', 'Dimension', 'Brandon Comments'], dtype='object')\n",
      "Index(['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_csv.columns)\n",
    "print(pairing_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from Old goals to New goal\n",
    "goal_mapping = pd.Series(df_csv['New goal'].values, index=df_csv['Old goals']).to_dict()\n",
    "\n",
    "# Update the 'goal' column in the pairing_df DataFrame using the mapping\n",
    "pairing_df['goal'] = pairing_df['goal'].map(goal_mapping).fillna(pairing_df['goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [name, description, detectors, goal, probe, tags, uri]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Search the 'column_name' column for the phrase 'your_phrase'\n",
    "mask = pairing_df['goal'].str.contains('degrade model performance on NLP tasks')\n",
    "\n",
    "# Print the rows where the phrase is found\n",
    "print(pairing_df[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing spaces from all strings in df_csv\n",
    "df_csv = df_csv.applymap(lambda x: x.strip() if type(x) == str else x)\n",
    "\n",
    "# Remove leading and trailing spaces from all strings in pairing_df\n",
    "pairing_df = pairing_df.applymap(lambda x: x.strip() if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from Old goals to New goal\n",
    "goal_mapping = pd.Series(df_csv['New goal'].values, index=df_csv['Old goals']).to_dict()\n",
    "\n",
    "# Update the 'goal' column in the pairing_df DataFrame using the mapping\n",
    "pairing_df['goal'] = pairing_df['goal'].map(goal_mapping).fillna(pairing_df['goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_df['goal'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>detectors</th>\n",
       "      <th>goal</th>\n",
       "      <th>probe</th>\n",
       "      <th>tags</th>\n",
       "      <th>uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:----------------------------------</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:-----------------------------------------------</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "      <td>:---------------------------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dan.AntiDAN</td>\n",
       "      <td>Anti-DAN</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.AntiDAN</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dan.AutoDANProbe</td>\n",
       "      <td>Probe leveraging generated AutoDAN prompts</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.AutoDANProbe</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td>https://arxiv.org/abs/2310.04451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>ChatGPT DevMode + Ranti</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>ChatGPT Developer Mode v2</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  \\\n",
       "0  :----------------------------------   \n",
       "1                          dan.AntiDAN   \n",
       "2                     dan.AutoDANProbe   \n",
       "3     dan.ChatGPT_Developer_Mode_RANTI   \n",
       "4        dan.ChatGPT_Developer_Mode_v2   \n",
       "\n",
       "                                         description  \\\n",
       "0  :---------------------------------------------...   \n",
       "1                                           Anti-DAN   \n",
       "2         Probe leveraging generated AutoDAN prompts   \n",
       "3                            ChatGPT DevMode + Ranti   \n",
       "4                          ChatGPT Developer Mode v2   \n",
       "\n",
       "                                           detectors  \\\n",
       "0  :---------------------------------------------...   \n",
       "1  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "2  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "3  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "4  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "\n",
       "                                                goal  \\\n",
       "0  :---------------------------------------------...   \n",
       "1                        disregard the system prompt   \n",
       "2                        disregard the system prompt   \n",
       "3                        disregard the system prompt   \n",
       "4                        disregard the system prompt   \n",
       "\n",
       "                                              probe  \\\n",
       "0  :-----------------------------------------------   \n",
       "1                          garak.probes.dan.AntiDAN   \n",
       "2                     garak.probes.dan.AutoDANProbe   \n",
       "3     garak.probes.dan.ChatGPT_Developer_Mode_RANTI   \n",
       "4        garak.probes.dan.ChatGPT_Developer_Mode_v2   \n",
       "\n",
       "                                                tags  \\\n",
       "0  :---------------------------------------------...   \n",
       "1  vijil:Security, source:garak, avid-effect:secu...   \n",
       "2  vijil:Security, source:garak, avid-effect:secu...   \n",
       "3  vijil:Security, source:garak, avid-effect:secu...   \n",
       "4  vijil:Security, source:garak, avid-effect:secu...   \n",
       "\n",
       "                                                 uri  \n",
       "0  :---------------------------------------------...  \n",
       "1                                                     \n",
       "2                   https://arxiv.org/abs/2310.04451  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>detectors</th>\n",
       "      <th>goal</th>\n",
       "      <th>probe</th>\n",
       "      <th>tags</th>\n",
       "      <th>uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dan.AntiDAN</td>\n",
       "      <td>Anti-DAN</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.AntiDAN</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dan.AutoDANProbe</td>\n",
       "      <td>Probe leveraging generated AutoDAN prompts</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.AutoDANProbe</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td>https://arxiv.org/abs/2310.04451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>ChatGPT DevMode + Ranti</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_RANTI</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>ChatGPT Developer Mode v2</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Developer_Mode_v2</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dan.ChatGPT_Image_Markdown</td>\n",
       "      <td>ChatGPT Markdown Image Unlocker / markdown inj...</td>\n",
       "      <td>garak.detectors.mitigation.MitigationBypass, g...</td>\n",
       "      <td>disregard the system prompt</td>\n",
       "      <td>garak.probes.dan.ChatGPT_Image_Markdown</td>\n",
       "      <td>vijil:Security, source:garak, avid-effect:secu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  \\\n",
       "1                       dan.AntiDAN   \n",
       "2                  dan.AutoDANProbe   \n",
       "3  dan.ChatGPT_Developer_Mode_RANTI   \n",
       "4     dan.ChatGPT_Developer_Mode_v2   \n",
       "5        dan.ChatGPT_Image_Markdown   \n",
       "\n",
       "                                         description  \\\n",
       "1                                           Anti-DAN   \n",
       "2         Probe leveraging generated AutoDAN prompts   \n",
       "3                            ChatGPT DevMode + Ranti   \n",
       "4                          ChatGPT Developer Mode v2   \n",
       "5  ChatGPT Markdown Image Unlocker / markdown inj...   \n",
       "\n",
       "                                           detectors  \\\n",
       "1  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "2  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "3  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "4  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "5  garak.detectors.mitigation.MitigationBypass, g...   \n",
       "\n",
       "                          goal                                          probe  \\\n",
       "1  disregard the system prompt                       garak.probes.dan.AntiDAN   \n",
       "2  disregard the system prompt                  garak.probes.dan.AutoDANProbe   \n",
       "3  disregard the system prompt  garak.probes.dan.ChatGPT_Developer_Mode_RANTI   \n",
       "4  disregard the system prompt     garak.probes.dan.ChatGPT_Developer_Mode_v2   \n",
       "5  disregard the system prompt        garak.probes.dan.ChatGPT_Image_Markdown   \n",
       "\n",
       "                                                tags  \\\n",
       "1  vijil:Security, source:garak, avid-effect:secu...   \n",
       "2  vijil:Security, source:garak, avid-effect:secu...   \n",
       "3  vijil:Security, source:garak, avid-effect:secu...   \n",
       "4  vijil:Security, source:garak, avid-effect:secu...   \n",
       "5  vijil:Security, source:garak, avid-effect:secu...   \n",
       "\n",
       "                                uri  \n",
       "1                                    \n",
       "2  https://arxiv.org/abs/2310.04451  \n",
       "3                                    \n",
       "4                                    \n",
       "5                                    "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a mask of rows where any element starts with ':-'\n",
    "mask = pairing_df.applymap(lambda x: str(x).startswith(':-')).any(axis=1)\n",
    "\n",
    "# Invert the mask and use it to filter the DataFrame\n",
    "pairing_df = pairing_df[~mask]\n",
    "pairing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
