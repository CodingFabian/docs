{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/Users/vigil/Desktop/Vigil/autoredteam/autoredteam/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def get_detectors(test_detector_dict):\n",
    "    extracted_values = {}\n",
    "    for key, values in test_detector_dict.items():\n",
    "        extracted_values[key] = [f\"{value.__class__.__module__}.{value.__class__.__name__}\" for value in values]\n",
    "    return extracted_values\n",
    "\n",
    "\n",
    "def get_counts(extracted_values):\n",
    "    values = [value for sublist in extracted_values.values() for value in sublist]\n",
    "    value_counts = Counter(values)\n",
    "\n",
    "    return value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OctoAI Agent: mistral-7b-instruct-fp16\n"
     ]
    }
   ],
   "source": [
    "from autoredteam.agents.octo import OctoAPI\n",
    "agent = OctoAPI(name = \"mistral-7b-instruct-fp16\", generations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from autoredteam.harnesses.dimension import SecurityHarness\n",
    "# , ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness\n",
    "harnesses = [SecurityHarness]\n",
    "harness_instances = [harness(agent) for harness in harnesses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize a dictionary to store the nodes and their parents\n",
    "nodes = defaultdict(list)\n",
    "\n",
    "# Iterate over the harness instances\n",
    "for i in harness_instances:\n",
    "    # Get the class name and module of the harness instance\n",
    "    i_name = i.__class__.__name__\n",
    "    i_module = i.__class__.__module__.split('.')[-1]\n",
    "\n",
    "    # Iterate over the test instances in each harness instance\n",
    "    for j in i.test_instances:\n",
    "        # Get the class name and module of the test instance\n",
    "        j_name = j.__class__.__name__\n",
    "        j_module = j.__class__.__module__.split('.')[-1]\n",
    "\n",
    "        # Add the test instance as a child of the harness instance\n",
    "        nodes[f'{i_module}.{i_name}'].append(f'{j_module}.{j_name}')\n",
    "\n",
    "        # Iterate over the detectors in each test instance\n",
    "        for k in j.detectors:\n",
    "            # Get the class name of the detector\n",
    "            k_name = k.__class__.__name__\n",
    "\n",
    "            # Add the detector as a child of the test instance\n",
    "            nodes[f'{j_module}.{j_name}'].append(k_name)\n",
    "\n",
    "# Create a list of tuples representing the nodes and their parents\n",
    "data = [(parent, child) for parent in nodes for child in nodes[parent]]\n",
    "\n",
    "# Flatten the data and encode the strings to numerical identifiers\n",
    "le = LabelEncoder()\n",
    "data_flat = le.fit_transform([item for sublist in data for item in sublist])\n",
    "\n",
    "# Reshape the data to its original shape\n",
    "data_encoded = data_flat.reshape(-1, 2)\n",
    "\n",
    "# Create a linkage matrix\n",
    "linkage_matrix = linkage(data_encoded, 'single')\n",
    "\n",
    "# Create labels for the pairs of data points\n",
    "labels = [f'{parent} -> {child}' for parent, child in data]\n",
    "\n",
    "# Create the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linkage_matrix, labels=labels, orientation='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree, TreeStyle, TextFace\n",
    "from collections import defaultdict\n",
    "# Initialize a dictionary to store the nodes and their parents\n",
    "nodes = defaultdict(list)\n",
    "\n",
    "# Iterate over the harness instances\n",
    "for i in harness_instances:\n",
    "    # Get the class name and module of the harness instance\n",
    "    i_name = i.__class__.__name__\n",
    "    i_module = i.__class__.__module__.split('.')[-1]\n",
    "\n",
    "    # Iterate over the test instances in each harness instance\n",
    "    for j in i.test_instances:\n",
    "        # Get the class name and module of the test instance\n",
    "        j_name = j.__class__.__name__\n",
    "        j_module = j.__class__.__module__.split('.')[-1]\n",
    "\n",
    "        # Add the test instance as a child of the harness instance\n",
    "        nodes[f'{i_module}.{i_name}'].append(f'{j_module}.{j_name}')\n",
    "\n",
    "        # Iterate over the detectors in each test instance\n",
    "        for k in j.detectors:\n",
    "            # Get the class name of the detector\n",
    "            k_name = k.__class__.__name__\n",
    "\n",
    "            # Add the detector as a child of the test instance\n",
    "            nodes[f'{j_module}.{j_name}'].append(k_name)\n",
    "\n",
    "# Create a new tree\n",
    "t = Tree()\n",
    "\n",
    "# Add nodes to the tree\n",
    "for parent, children in nodes.items():\n",
    "    parent_node = t.search_nodes(name=parent)\n",
    "    if not parent_node:\n",
    "        parent_node = t.add_child(name=parent)\n",
    "    else:\n",
    "        parent_node = parent_node[0]\n",
    "    for child in children:\n",
    "        parent_node.add_child(name=child)\n",
    "\n",
    "# Create a tree style\n",
    "ts = TreeStyle()\n",
    "ts.show_leaf_name = False\n",
    "\n",
    "# Add labels to the nodes\n",
    "for node in t.traverse():\n",
    "    node.add_face(TextFace(node.name), column=0, position=\"branch-right\")\n",
    "\n",
    "# Show the tree\n",
    "t.render(\"%%inline\", tree_style=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/vigil/anaconda3/envs/art/lib/python3.11/site-packages/ete3-3.1.3-py3.10.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: PyQt5 in /Users/vigil/anaconda3/envs/art/lib/python3.11/site-packages (5.15.10)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.13 in /Users/vigil/anaconda3/envs/art/lib/python3.11/site-packages (from PyQt5) (12.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install PyQt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [instance.test_instances for instance in harness_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for each element in sec_tests and store the dictionaries in another list\n",
    "test_detector_dicts = [{str(test): test.detectors for test in sec_test} for sec_test in tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detector_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_list = list(test_detector_dicts[1].keys())\n",
    "tester = keys_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib.util\n",
    "import inspect\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/vigil/Desktop/Vigil/autoredteam')\n",
    "\n",
    "# Define the directory with the .py files\n",
    "directory = '/Users/vigil/Desktop/Vigil/autoredteam/autoredteam/tests'\n",
    "\n",
    "# Define the markdown file\n",
    "markdown_directory = 'markdown_files'\n",
    "markdown_file = os.path.join(markdown_directory, 'attributes.md')\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(markdown_directory, exist_ok=True)\n",
    "# Open the markdown file\n",
    "with open(markdown_file, 'w') as f:\n",
    "    # List all .py files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.py'):\n",
    "            # Import the .py file as a module\n",
    "            spec = importlib.util.spec_from_file_location(filename[:-3], os.path.join(directory, filename))\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "\n",
    "            # Get the attributes of the module\n",
    "            attributes = inspect.getmembers(module)\n",
    "\n",
    "            # Filter the attributes\n",
    "            attributes = [attr for attr in attributes if attr[0] in ['__doc__', 'detectors', 'uri', 'name', 'description', 'tags', 'goal']]\n",
    "\n",
    "            # Write the attributes to the markdown file in a table format\n",
    "            f.write(f'## {filename[:-3]}\\n')\n",
    "            f.write('| Attribute | Value |\\n')\n",
    "            f.write('| --- | --- |\\n')\n",
    "            for attr in attributes:\n",
    "                f.write(f'| {attr[0]} | {attr[1]} |\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the markdown file\n",
    "markdown_file = 'markdown_files/all_tests.md'\n",
    "\n",
    "# Create a directory for the markdown file\n",
    "os.makedirs(os.path.dirname(markdown_file), exist_ok=True)\n",
    "\n",
    "# Create a dictionary to group the subtests by their categories\n",
    "tests_by_category = defaultdict(list)\n",
    "\n",
    "# Iterate over all dictionaries in test_detector_dicts\n",
    "for test_detector_dict in test_detector_dicts:\n",
    "    # Iterate over the items in each dictionary\n",
    "    for test, detectors in test_detector_dict.items():\n",
    "        # Extract the relevant part of the test name\n",
    "        test_category = str(test).split('.')[2]\n",
    "        test_name = str(test).split(' ')[0].split('.')[-1]\n",
    "\n",
    "        # Group the subtests by their categories\n",
    "        tests_by_category[test_category].append((test_name, detectors))\n",
    "\n",
    "# Open the markdown file\n",
    "with open(markdown_file, 'w') as f:\n",
    "    # Iterate over the categories\n",
    "    for category, tests in tests_by_category.items():\n",
    "        # Write the category to the file\n",
    "        f.write(f'## {category}\\n')\n",
    "\n",
    "        # Write the subtests to the file\n",
    "        for test_name, detectors in tests:\n",
    "            f.write(f'- {test_name}\\n')\n",
    "\n",
    "            # Write the detectors to the file\n",
    "            for detector in detectors:\n",
    "                # Extract the relevant part of the detector name\n",
    "                detector_name = '.'.join(str(detector).split(' ')[0].split('.')[1:])\n",
    "                f.write(f'  - {detector_name}\\n')\n",
    "\n",
    "        # Add a newline for readability\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update each dictionary with the result of calling get_detectors on it\n",
    "for i in range(len(test_detector_dicts)):\n",
    "    test_detector_dicts[i] = get_detectors(test_detector_dicts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detector_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the markdown file\n",
    "markdown_file = 'markdown_files/all_tests.md'\n",
    "\n",
    "# Create a directory for the markdown file\n",
    "os.makedirs(os.path.dirname(markdown_file), exist_ok=True)\n",
    "\n",
    "# Open the markdown file\n",
    "with open(markdown_file, 'w') as f:\n",
    "    # Iterate over the dictionary in test_detector_dicts\n",
    "    for test, detectors in test_detector_dicts[0].items():\n",
    "        # Extract the test name from the object string\n",
    "        test_name = str(test).split(' ')[0].split('.')[-1]\n",
    "\n",
    "        # Write the test name to the file\n",
    "        f.write(f'## {test_name}\\n')\n",
    "\n",
    "        # Write the detectors to the file\n",
    "        for detector in detectors:\n",
    "            f.write(f'- {detector}\\n')\n",
    "\n",
    "        # Add a newline for readability\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counts = [get_counts(dict) for dict in test_detector_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness_detector_counts= list(zip(harnesses, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness_detector_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Iterate over the zipped list\n",
    "for harness, counter in harness_detector_counts:\n",
    "    # Get the names of the detectors and their counts\n",
    "    detectors = list(counter.keys())\n",
    "    counts = list(counter.values())\n",
    "    \n",
    "    # Sort the detectors and counts by the counts\n",
    "    detectors, counts = zip(*sorted(zip(detectors, counts), key=lambda x: x[1]))\n",
    "    \n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(10, 5))  # Set the figure size\n",
    "    plt.barh(detectors, counts)  # Create a horizontal bar chart\n",
    "    plt.title(str(harness))  # Set the title to the name of the harness\n",
    "    plt.xlabel('Counts')  # Set the x-axis label\n",
    "    plt.ylabel('Detectors')  # Set the y-axis label\n",
    "    plt.xlim(0, max(counts) + 1)  # Set the x-axis limits\n",
    "    plt.xticks(np.arange(0, max(counts) + 1, step=1))  # Set the x-axis ticks to integers\n",
    "    plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.harnesses.dimension import * \n",
    "harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "harness_instances = [harness(agent) for harness in harnesses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.detectors.base import *\n",
    "detectors = [Detector, HFDetector, StringAbsenceDetector, StringDetector, TriggerListAbsenceDetector, TriggerListDetector]\n",
    "first_instance = detectors[0]()\n",
    "\n",
    "# Get all the attributes and methods of the instance\n",
    "attributes = dir(first_instance)\n",
    "\n",
    "# Print the attributes\n",
    "for attribute in attributes:\n",
    "    print(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.detectors.base import *\n",
    "detectors = [Detector, StringDetector, TriggerListAbsenceDetector, TriggerListDetector]\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['bcp47', 'description', 'detect', 'detectorname', 'name', 'precision', 'recall', 'tags', 'uri']\n",
    "\n",
    "for DetectorClass in detectors:\n",
    "    # Create an instance of the class\n",
    "    instance = DetectorClass()\n",
    "\n",
    "    # Extract the specified attributes\n",
    "    for attribute in attributes_to_extract:\n",
    "        if hasattr(instance, attribute):\n",
    "            print(f'{attribute}: {getattr(instance, attribute)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "# List of attributes to look for\n",
    "attributes_to_look_for = ['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri']\n",
    "\n",
    "instances_data = []\n",
    "for instance in test.test_instances:\n",
    "    instance_dict = {}\n",
    "    for attribute in attributes_to_look_for:\n",
    "        if hasattr(instance, attribute):\n",
    "            if attribute == 'detectors':\n",
    "                # Get the class name of each detector and join them into a single string\n",
    "                instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "            elif attribute == 'probe':\n",
    "                # Get the class name of the probe\n",
    "                probe = getattr(instance, attribute)\n",
    "                instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "            elif attribute == 'tags':\n",
    "                # Join the tags into a single string\n",
    "                instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "            else:\n",
    "                instance_dict[attribute] = getattr(instance, attribute)\n",
    "    instances_data.append(instance_dict)\n",
    "\n",
    "print(instances_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Add a title to the markdown\n",
    "markdown_table = '# Security\\n' + markdown_table\n",
    "\n",
    "print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.harnesses.dimension import * \n",
    "from tabulate import tabulate\n",
    "\n",
    "harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "harness_instances = [harness(agent) for harness in harnesses]\n",
    "\n",
    "# List of attributes to look for\n",
    "attributes_to_look_for = ['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri']\n",
    "\n",
    "for harness_instance in harness_instances:\n",
    "    instances_data = []\n",
    "    for instance in harness_instance.test_instances:\n",
    "        instance_dict = {}\n",
    "        for attribute in attributes_to_look_for:\n",
    "            if hasattr(instance, attribute):\n",
    "                if attribute == 'detectors':\n",
    "                    # Get the class name of each detector and join them into a single string\n",
    "                    instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "                elif attribute == 'probe':\n",
    "                    # Get the class name of the probe\n",
    "                    probe = getattr(instance, attribute)\n",
    "                    instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "                elif attribute == 'tags':\n",
    "                    # Join the tags into a single string\n",
    "                    instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "                elif attribute == 'description':\n",
    "                    # Check if the description is a tuple, and if so, take the first element\n",
    "                    description = getattr(instance, attribute)\n",
    "                    if isinstance(description, tuple):\n",
    "                        description = description[0]\n",
    "                    instance_dict[attribute] = description\n",
    "                else:\n",
    "                    instance_dict[attribute] = getattr(instance, attribute)\n",
    "        instances_data.append(instance_dict)\n",
    "\n",
    "    # Convert the list of dictionaries to a markdown table\n",
    "    markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "    # Add a title to the markdown\n",
    "    markdown_table = '# ' + type(harness_instance).__name__ + '\\n' + markdown_table\n",
    "\n",
    "    print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.harnesses.dimension import * \n",
    "from tabulate import tabulate\n",
    "\n",
    "def generate_documentation(attributes_to_look_for, save_path):\n",
    "    harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "    harness_instances = [harness(agent) for harness in harnesses]\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        for harness_instance in harness_instances:\n",
    "            instances_data = []\n",
    "            for instance in harness_instance.test_instances:\n",
    "                instance_dict = {}\n",
    "                for attribute in attributes_to_look_for:\n",
    "                    if hasattr(instance, attribute):\n",
    "                        if attribute == 'detectors':\n",
    "                            # Get the class name of each detector and join them into a single string\n",
    "                            instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "                        elif attribute == 'probe':\n",
    "                            # Get the class name of the probe\n",
    "                            probe = getattr(instance, attribute)\n",
    "                            instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "                        elif attribute == 'tags':\n",
    "                            # Join the tags into a single string\n",
    "                            instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "                        else:\n",
    "                            instance_dict[attribute] = getattr(instance, attribute)\n",
    "                instances_data.append(instance_dict)\n",
    "\n",
    "            # Convert the list of dictionaries to a markdown table\n",
    "            markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "            # Add a title to the markdown\n",
    "            markdown_table = '# ' + type(harness_instance).__name__ + '\\n' + markdown_table\n",
    "\n",
    "            # Write the markdown table to the file\n",
    "            f.write(markdown_table + '\\n\\n')\n",
    "\n",
    "# Call the function with the attributes to look for and the save path\n",
    "generate_documentation(['name', 'description', 'detectors', 'goal', 'probe', 'tags', 'uri'], 'vijil_detectors.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from autoredteam.harnesses.dimension import * \n",
    "from tabulate import tabulate\n",
    "\n",
    "def generate_documentation(attributes_to_look_for, save_path):\n",
    "    harnesses = [SecurityHarness, ToxicityHarness, PrivacyHarness, HallucinationHarness, RobustnessHarness, ToxicityHarness, StereotypeHarness, FairnessHarness, EthicsHarness]\n",
    "    harness_instances = [harness(agent) for harness in harnesses]\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        for harness_instance in harness_instances:\n",
    "            instances_data = []\n",
    "            for instance in harness_instance.test_instances:\n",
    "                instance_dict = {}\n",
    "                for attribute in attributes_to_look_for:\n",
    "                    if hasattr(instance, attribute):\n",
    "                        if attribute == 'detectors':\n",
    "                            # Get the class name of each detector and join them into a single string\n",
    "                            instance_dict[attribute] = ', '.join([type(detector).__module__ + '.' + type(detector).__name__ for detector in getattr(instance, attribute)])\n",
    "                        elif attribute == 'probe':\n",
    "                            # Get the class name of the probe\n",
    "                            probe = getattr(instance, attribute)\n",
    "                            instance_dict[attribute] = type(probe).__module__ + '.' + type(probe).__name__\n",
    "                        elif attribute == 'tags':\n",
    "                            # Join the tags into a single string\n",
    "                            instance_dict[attribute] = ', '.join(getattr(instance, attribute))\n",
    "                        elif attribute == 'description':\n",
    "                            # Check if the description is a tuple, and if so, take the first element\n",
    "                            description = getattr(instance, attribute)\n",
    "                            if isinstance(description, tuple):\n",
    "                                description = description[0]\n",
    "                            instance_dict[attribute] = description\n",
    "                        else:\n",
    "                            instance_dict[attribute] = getattr(instance, attribute)\n",
    "                instances_data.append(instance_dict)\n",
    "\n",
    "            # Convert the list of dictionaries to a markdown table\n",
    "            markdown_table = tabulate(instances_data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "            # Add a title to the markdown\n",
    "            markdown_table = '# ' + type(harness_instance).__name__ + '\\n' + markdown_table\n",
    "\n",
    "            # Write the markdown table to the file\n",
    "            f.write(markdown_table + '\\n\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Generate documentation for test instances.')\n",
    "    parser.add_argument('attributes', type=str, nargs='+', help='List of attributes to generate documentation for.')\n",
    "    parser.add_argument('save_path', type=str, help='Path to save the markdown file.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Call the function with the attributes to look for and the save path\n",
    "    generate_documentation(args.attributes, args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from garak.detectors.dan import DAN, DANJailbreak, DevMode, DUDE, STAN, AntiDAN, MarkdownLink\n",
    "from tabulate import tabulate\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['bcp47', 'description', 'detect', 'detectorname', 'name', 'precision', 'recall', 'tags', 'uri']\n",
    "\n",
    "instances = [DAN, DANJailbreak, DevMode, DUDE, STAN, AntiDAN, MarkdownLink]\n",
    "\n",
    "data = []\n",
    "\n",
    "# Extract the specified attributes\n",
    "for instance in instances:\n",
    "    row = {'name': instance.__name__}\n",
    "    for attribute in attributes_to_extract:\n",
    "        if hasattr(instance, attribute):\n",
    "            row[attribute] = getattr(instance, attribute)\n",
    "    data.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Save the markdown table to a file\n",
    "with open('output.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import garak.detectors as detectors\n",
    "from tabulate import tabulate\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['detectorname', 'name', 'bcp47', 'description', 'detect', 'tags', 'uri']\n",
    "\n",
    "# Get all modules in the garak.detectors package\n",
    "detector_modules = list(pkgutil.iter_modules(detectors.__path__))\n",
    "\n",
    "instances = []\n",
    "\n",
    "# Import each module and add its classes to the instances list\n",
    "for _, module_name, _ in detector_modules:\n",
    "    module = __import__(f'garak.detectors.{module_name}', fromlist=[module_name])\n",
    "    for attribute_name in dir(module):\n",
    "        attribute = getattr(module, attribute_name)\n",
    "        if isinstance(attribute, type) and attribute_name != 'Attempt':\n",
    "            instances.append(attribute)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Extract the specified attributes\n",
    "for instance in instances:\n",
    "    row = {'name': instance.__name__}\n",
    "    for attribute in attributes_to_extract:\n",
    "        if hasattr(instance, attribute):\n",
    "            row[attribute] = getattr(instance, attribute)\n",
    "    data.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Save the markdown table to a file\n",
    "with open('new_output.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from garak.detectors.riskywords import SurgeProfanityAnimalReferences\n",
    "this = SurgeProfanityAnimalReferences()\n",
    "this.substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import garak.detectors as detectors\n",
    "from tabulate import tabulate\n",
    "import types\n",
    "import json\n",
    "\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, type):\n",
    "            return str(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "# List of attributes to extract\n",
    "attributes_to_extract = ['detectorname', 'name', 'bcp47', 'description', 'detect', 'tags', 'uri', 'substrings']\n",
    "\n",
    "# Get all modules in the garak.detectors package\n",
    "detector_modules = list(pkgutil.iter_modules(detectors.__path__))\n",
    "\n",
    "instances = []\n",
    "\n",
    "# Import each module and add its classes to the instances list\n",
    "for _, module_name, _ in detector_modules:\n",
    "    module = __import__(f'garak.detectors.{module_name}', fromlist=[module_name])\n",
    "    for attribute_name in dir(module):\n",
    "        attribute = getattr(module, attribute_name)\n",
    "        if isinstance(attribute, type) and attribute_name != 'Attempt':\n",
    "            try:\n",
    "                instances.append(attribute())\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "data = []\n",
    "\n",
    "# Extract the specified attributes\n",
    "for instance in instances:\n",
    "    row = {'name': instance.__class__.__name__}\n",
    "    if row['name'] not in ['Detector', 'StringDetector', 'TriggerListDetector', 'defaultdict']:\n",
    "        for attribute in attributes_to_extract:\n",
    "            if hasattr(instance, attribute):\n",
    "                attr_value = getattr(instance, attribute)\n",
    "                if isinstance(attr_value, types.MethodType):\n",
    "                    row[attribute] = f'{instance.__class__.__name__}.{attribute}'\n",
    "                elif isinstance(attr_value, list):\n",
    "                    row[attribute] = ', '.join(attr_value)\n",
    "                else:\n",
    "                    row[attribute] = attr_value\n",
    "            else:\n",
    "                if attribute == 'substrings':\n",
    "                    row[attribute] = 'N/A'\n",
    "        data.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a markdown table\n",
    "markdown_table = tabulate(data, headers=\"keys\", tablefmt=\"pipe\")\n",
    "\n",
    "# Save the markdown table to a file\n",
    "with open('detectors.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        return str(obj)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('detector.json', 'w') as f:\n",
    "    json.dump(data, f, cls=CustomEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this.substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoredteam.detectors import adultdata, advstereo, base, hallucination, paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_detector_info(modules_to_inspect):\n",
    "    # Initialize a list to store the detector info\n",
    "    detector_info = []\n",
    "\n",
    "    # Initialize a set to store the names of the saved detectors\n",
    "    saved_detectors = set()\n",
    "\n",
    "    # Iterate over each module\n",
    "    for module_name in modules_to_inspect:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "\n",
    "        # Get all classes in the module\n",
    "        classes = inspect.getmembers(module, inspect.isclass)\n",
    "\n",
    "        # Iterate over each class\n",
    "        for class_name, class_ in classes:\n",
    "            # Get all subclasses of the class\n",
    "            subclasses = class_.__subclasses__()\n",
    "\n",
    "            # Iterate over each subclass\n",
    "            for subclass in subclasses:\n",
    "                # Check if the detector has already been saved\n",
    "                if subclass.__name__ in saved_detectors:\n",
    "                    continue\n",
    "\n",
    "                # Add the detector to the set of saved detectors\n",
    "                saved_detectors.add(subclass.__name__)\n",
    "\n",
    "                # Try to create an instance of the subclass\n",
    "                try:\n",
    "                    instance = subclass()\n",
    "                except TypeError:\n",
    "                    instance = None\n",
    "\n",
    "                # Get the subclass info\n",
    "                info = {\n",
    "                    \"Detector\": subclass.__name__,\n",
    "                    \"Module\": subclass.__module__,\n",
    "                    \"Description\": inspect.getdoc(subclass),\n",
    "                    \"Substrings\": instance.substrings if instance and hasattr(instance, 'substrings') else \"N/A\"\n",
    "                }\n",
    "                # Add the subclass info to the detector info list\n",
    "                detector_info.append(info)\n",
    "\n",
    "    # Write the detector info to a JSON file\n",
    "    with open('art_detectors.json', 'w') as f:\n",
    "        json.dump(detector_info, f, indent=4)\n",
    "\n",
    "    # Write the detector info to a Markdown file\n",
    "    with open('art_detectors.md', 'w') as f:\n",
    "        # Write the table headers\n",
    "        f.write(\"| Detector | Module | Description | Substrings |\\n\")\n",
    "        f.write(\"| -------- | ------ | ----------- | ---------- |\\n\")\n",
    "\n",
    "        # Write the table data\n",
    "        for info in detector_info:\n",
    "            f.write(f\"| {info['Detector']} | {info['Module']} | {info['Description']} | {info['Substrings']} |\\n\")\n",
    "\n",
    "# List of modules to inspect\n",
    "modules_to_inspect = ['autoredteam.detectors.adultdata', 'autoredteam.detectors.advstereo', \n",
    "                      'autoredteam.detectors.hallucination', \n",
    "                      'autoredteam.detectors.paraphrase']\n",
    "\n",
    "write_detector_info(modules_to_inspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "def get_module_info(module_name):\n",
    "    # Import the module\n",
    "    module = importlib.import_module(module_name)\n",
    "\n",
    "    # Get all classes in the module\n",
    "    classes = inspect.getmembers(module, inspect.isclass)\n",
    "\n",
    "    # Initialize a list to store the class info\n",
    "    class_info = []\n",
    "\n",
    "    # Iterate over each class\n",
    "    for class_name, class_ in classes:\n",
    "        # Get the class info\n",
    "        info = {\n",
    "            \"Detector\": class_name,\n",
    "            \"Module\": class_.__module__,\n",
    "            \"Description\": inspect.getdoc(class_),\n",
    "            \"Substrings\": \"N/A\"\n",
    "        }\n",
    "        # Add the class info to the class info list\n",
    "        class_info.append(info)\n",
    "\n",
    "    return class_info\n",
    "\n",
    "# List of modules to inspect\n",
    "modules_to_inspect = ['garak.detectors.base']\n",
    "\n",
    "# Iterate over each module\n",
    "for module_name in modules_to_inspect:\n",
    "    # Get the module info\n",
    "    module_info = get_module_info(module_name)\n",
    "\n",
    "    # Write the module info to a JSON file\n",
    "    with open('module_info.json', 'w') as f:\n",
    "        json.dump(module_info, f, indent=4)\n",
    "\n",
    "    # Write the module info to a Markdown file\n",
    "    with open('module_info.md', 'w') as f:\n",
    "        # Write the table headers\n",
    "        f.write(\"| Detector | Module | Description | Substrings |\\n\")\n",
    "        f.write(\"| -------- | ------ | ----------- | ---------- |\\n\")\n",
    "\n",
    "        # Write the table data\n",
    "        for info in module_info:\n",
    "            f.write(f\"| {info['Detector']} | {info['Module']} | {info['Description']} | {info['Substrings']} |\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from io import StringIO\n",
    "\n",
    "# Path to your markdown file\n",
    "md_file_path = 'test_detector_pairing.md'\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "data = pd.DataFrame(columns=['harness', 'name', 'detectors'])\n",
    "\n",
    "# Read markdown file and parse data\n",
    "with open(md_file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    harness = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('#'):\n",
    "            harness = line.strip().lstrip('# ')\n",
    "        elif line.startswith('| name'):\n",
    "            end = [j for j in range(i, len(lines)) if lines[j].startswith('|:---')]\n",
    "            md_data = ''.join(lines[i:end[0]+3])  # Adjusted this line to read all rows of the table\n",
    "            df = pd.read_csv(StringIO(md_data), sep=\"|\")\n",
    "            df.columns = df.columns.str.strip()\n",
    "            df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "            df = df.assign(detectors=df['detectors'].str.split(',')).explode('detectors')\n",
    "            df['detectors'] = df['detectors'].str.strip()\n",
    "            df['harness'] = harness\n",
    "            data = pd.concat([data, df])\n",
    "\n",
    "# Create a linkage matrix\n",
    "le = LabelEncoder()\n",
    "data['label'] = data['harness'] + ' -> ' + data['name'] + ' -> ' + data['detectors']\n",
    "linkage_matrix = linkage(le.fit_transform(data['label']).reshape(-1, 1), 'single')\n",
    "\n",
    "# Create the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linkage_matrix, labels=data['label'].values, orientation='right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
