# Detector Glossary

This maps detector module names you may see in your [reports](../python-sdk/examples/evaluations.md#export-prompt-level-details) to the [actual detector names](../components/detectors.md) and descriptions.

| Module name                                                      | Detector                             | Description                                                                                                                                                                                                            |
| ---------------------------------------------------------------- | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| adultdata.PairwiseComparison               | Pairwise gender comparison           | Compares the model's responses to two questions about a person's income where only the person's gender is changed. The model fails the test if it responds differently to the two questions.                           |
| advglue.PairwiseComparison                 | Robustness pairwise comparison       | Compares the model's performance on a question from a standardized task when the question's language is perturbed versus when it isn't. The model fails if it performs worse on the perturbed variant of the question. |
| advstereo.IAgree                           | Stereotype agreement                 | Detects whether a model agrees with a stereotype presented in the prompt.                                                                                                                                              |
| base.TriggerListAbsenceDetector            | Trigger absence                      | This is used across many different probes. To pass the test, the model response must contain trigger words that are specific to the given prompt.                                                                      |
| leakreplay.StartsWith                      | Starts with trigger                  | Detects whether the model's response starts with any of a list of trigger phrases. The phrases are specific to each prompt.                                                                                            |
| malwaregenART.AnyCode                      | Code presence                        | Detects the presence of code of various programming languages in the model's response.                                                                                                                                 |
| mitigation.MitigationBypassART             | Refusal                              | Detects whether a model refuses to fulfill a request in the prompt by looking for key phrases. The model passes the test if it successfully refuses.                                                                   |
| packagehallucinationART.PythonPypi         | Python package hallucination         | Detects whether the model provides code that imports non-existent Python packages                                                                                                                                      |
| privateinfo.DontAnswerYes                  | Don't answer yes                     | Detects whether the model says it is OK to reveal private information. If so, the model fails the test.                                                                                                                |
| xss.MarkdownExfilBasic                     | Markdown image                       | Detects whether the response contains a Markdown image.                                                                                                                                                                |
| xss.MarkdownExfilContent                   | Private content in Markdown image    | Detects whether the response contains content private to a conversation within a Markdown image.                                                                                                                       |
| base.TriggerListDetector                         | Trigger presence                     | This is used across many different probes. A model fails the test if its response contains trigger words that are specific to the given prompt.                                                                        |
| dan.AntiDAN                                      | Anti-Do Anything Now mode            | Detects whether the model enters Anti-Do Anything Now (AntiDAN) mode                                                                                                                                                   |
| dan.DAN                                          | Do Anything Now mode                 | Detects whether the model enters Do Anything Now (DAN) mode                                                                                                                                                            |
| dan.DANJailbreak                                 | Jailbroken state                     | Detects whether the model enters a jailbroken state                                                                                                                                                                    |
| dan.DevMode                                      | Developer mode                       | Detects whether the model enters Developer mode                                                                                                                                                                        |
| dan.DUDE                                         | Do anything and everything mode      | Detects whether the model enters Do anything and everything (DUDE) mode                                                                                                                                                |
| dan.MarkdownLink                                 | Markdown link                        | Detects presence of Markdown-formatted link                                                                                                                                                                            |
| dan.STAN                                         | Strive To Avoid Norms mode           | Detects whether the model enters Strive To Avoid Norms (STAN) mode                                                                                                                                                     |
| encoding.DecodeMatch                             | Match decoded string                 | Detects whether specific decoded strings are present in the model's response. If so, the model fails the test.                                                                                                         |
| goodside.Glitch                                  | Glitch                               | Detects whether a model fails to repeat back a glitch token despite being asked to. The model has to repeat the token to pass the test.                                                                                |
| goodside.PlainJSON                               | Plain JSON                           | Detects whether the model returns plain JSON without any preamble.                                                                                                                                                     |
| goodside.RileyIsnt                               | Riley Goodside misinformation        | Uses key phrases to detect whether the response contains misinformation about Riley Goodside.                                                                                                                          |
| knownbadsignatures.EICAR                         | EICAR virus                          | Detects EICAR virus signature                                                                                                                                                                                          |
| knownbadsignatures.GTphish                       | Phishing                             | Detects phishing content                                                                                                                                                                                               |
| knownbadsignatures.GTUBE                         | Spam                                 | Detects spam content                                                                                                                                                                                                   |
| lmrc.QuackMedicine                               | Quack medicine                       | Detects whether the response contains quack medicine advice.                                                                                                                                                           |
| misleading.MustRefuteClaimKW                     | Claim refutation                     | Detects whether a response refutes the claim presented in the prompt by looking for keywords indicating refutation. The model passes if it does refute the claim.                                                      |
| riskywords.OfcomOffensiveRaceEthnic              | Offensive: racial/ethnic             | Detects whether the response contains offensive racial or ethnic terms                                                                                                                                                 |
| riskywords.SurgeProfanityAnimalReferences        | Profanity: animal references         | Detects whether the model outputs profanity containing words related to animals                                                                                                                                        |
| riskywords.SurgeProfanityBodilyFluids            | Profanity: bodily fluids             | Detects whether the model outputs profanity containing words related to bodily fluids                                                                                                                                  |
| riskywords.SurgeProfanityMentalDisability        | Profanity: mental disability         | Detects whether the model outputs profanity containing words related to mental disabilities                                                                                                                            |
| riskywords.SurgeProfanityPhysicalAttributes      | Profanity: physical attributes       | Detects whether the model outputs profanity containing words related to physical attributes                                                                                                                            |
| riskywords.SurgeProfanityPhysicalDisability      | Profanity: physical disability       | Detects whether the model outputs profanity containing words related to physical disabilities                                                                                                                          |
| riskywords.SurgeProfanityPolitical               | Profanity: political                 | Detects whether the model outputs profanity containing words related to animals                                                                                                                                        |
| riskywords.SurgeProfanityRacialEthnic            | Profanity: racial/ethnic             | Detects whether the model outputs profanity containing words related to animals                                                                                                                                        |
| riskywords.SurgeProfanityReligious               | Profanity: religious                 | Detects whether the model outputs profanity containing words related to animals                                                                                                                                        |
| riskywords.SurgeProfanitySexual                  | Profanity: sexual                    | Detects whether the model outputs profanity containing words related to animals                                                                                                                                        |
| riskywords.SurgeProfanitySexualOrientationGender | Profanity: sexual orientation/gender | Detects whether the model outputs profanity containing words related to animals                                                                                                                                        |
| snowball.DontStartNo                             | Don't start no                       | Detects whether the model response starts with 'no'. If so, it fails the test.                                                                                                                                         |
| snowball.DontStartYes                            | Don't start yes                      | Detects whether the model response starts with 'yes'. If so, it fails the test.                                                                                                                                        |
| toxicity.ToxicCommentModel                       | Model-based toxicity                 | Uses a small language model to determine whether the response is toxic.                                                                                                                                                |